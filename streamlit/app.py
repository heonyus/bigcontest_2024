import streamlit as st

st.set_page_config(page_title="ğŸŠ ì œì£¼ ë§›ì§‘ ì¶”ì²œ ì±—ë´‡", page_icon="ğŸŠ", layout="wide")

import os
os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'
import numpy as np
import pandas as pd
import torch
import faiss
import requests
import folium
from streamlit_folium import st_folium
from transformers import AutoTokenizer, AutoModel
import google.generativeai as genai
from tqdm import tqdm
import logging
import sys
import time as time_module
from datetime import datetime


# ë¡œê¹… ë ˆë²¨ ì„¤ì •ì„ DEBUGë¡œ ë³€ê²½
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),  # stdoutìœ¼ë¡œ ë³€ê²½
        logging.FileHandler("app_log.log", encoding='utf-8')  # ì¸ì½”ë”© ì§€ì •
    ]
)

# 1. ê¸°ë³¸ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
KAKAO_API_KEY = st.secrets["KAKAO_API_KEY"]

genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel("gemini-1.5-flash")

# transformers import ìˆ˜ì •
try:
    from transformers import AutoTokenizer, AutoModel
    logging.info("Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    logging.error(f"Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
    st.error("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œì— ì‹¤íŒ¨í–ˆ. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
    st.stop()

# ëª¨ë¸ ë¡œë“œ ë¶€ë¶„ ìˆ˜ì •
try:
    embedding_model_name = "jhgan/ko-sroberta-multitask"
    tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)
    embedding_model = AutoModel.from_pretrained(embedding_model_name)
    
    if torch.cuda.is_available():
        embedding_model = embedding_model.to(device)
        logging.info("ëª¨ë¸ì„ GPUë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    else:
        logging.info("ëª¨ë¸ì„ CPUë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    logging.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    st.error("ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
    st.stop()

embedding_data_path = './data/JEJU_with_embeddings_final.csv'

# ì„ë² ë”© ë¬¸ìì—´ì„ numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def str_to_array(embedding_str):
    try:
        # ë¬¸ìì—´ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
        numbers = [float(x) for x in embedding_str.strip('[]').split()]
        return np.array(numbers)
    except Exception as e:
        logging.error(f"ì„ë² ë”© ë³€í™˜ ì˜¤ë¥˜: {e}")
        return np.zeros(768)  # ê¸°ë³¸ ì„ë²  ì°¨

# 2. FAISS ì¸ë±ìŠ¤ ìƒì„± í•¨ìˆ˜ (CPU ì „ìš©)
def create_faiss_index(embeddings):
    try:
        dimension = embeddings.shape[1]
        logging.info(f"FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹œì‘ (ì°¨ì›: {dimension})")
        
        # CPU ë²„ì „ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
        index = faiss.IndexFlatL2(dimension)
        index.add(embeddings)
        
        logging.info("FAISS CPU ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        return index
    except Exception as e:
        logging.error(f"FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        raise

# ì•± ì‹œì‘ ë¶€ë¶„ì— ì¶”ê°€
@st.cache_resource
def load_models():
    """ëª¨ë¸ ë¡œë“œë¥¼ ìºì‹œí•˜ì—¬ ì¬ì‚¬ìš©"""
    try:
        embedding_model_name = "jhgan/ko-sroberta-multitask"
        tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)
        embedding_model = AutoModel.from_pretrained(embedding_model_name)
        
        if torch.cuda.is_available():
            embedding_model = embedding_model.to(device)
            logging.info("ëª¨ë¸ì„ GPUë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        else:
            logging.info("ëª¨ë¸ì„ CPUë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            
        return tokenizer, embedding_model
    except Exception as e:
        logging.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.error("ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
        st.stop()

@st.cache_data
def load_and_process_data():
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ë¥¼ ìºì‹œí•˜ì—¬ ì¬ì‚¬ìš©"""
    df = pd.read_csv(embedding_data_path, encoding='utf-8-sig')
    
    # ì¥ì†Œì™€ ì¹´í…Œê³ ë¦¬ ì„ë² ë”©ì„ ë³„ë„ì˜ FAISS ì¸ë±ìŠ¤ë¡œ ìƒì„±
    location_embeddings = np.stack(df['ì¥ì†Œ_ì„ë² ë”©'].apply(str_to_array).to_numpy()).astype(np.float32)
    category_embeddings = np.stack(df['ì¹´í…Œê³ ë¦¬_ì„ë² ë”©'].apply(str_to_array).to_numpy()).astype(np.float32)
    
    # L2 ì •ê·œí™” ì ìš©
    faiss.normalize_L2(location_embeddings)
    faiss.normalize_L2(category_embeddings)
    
    # ê°ê°ì˜ FAISS ì¸ë±ìŠ¤ ìƒì„±
    index_location = faiss.IndexFlatIP(location_embeddings.shape[1])
    index_category = faiss.IndexFlatIP(category_embeddings.shape[1])
    
    index_location.add(location_embeddings)
    index_category.add(category_embeddings)
    
    return df, index_location, index_category

# ë©”ì¸ ì½”ë“œì—ì„œ ì‚¬ìš©
tokenizer, embedding_model = load_models()
df, index_location, index_category = load_and_process_data()

def embed_text(text):
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)
    with torch.no_grad():
        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)
    return embeddings.cpu().numpy()[0]

def search_with_faiss(location, category, top_k=5):
    logging.info(f"=== FAISS ê²€ìƒ‰ ì‹œì‘ ===")
    logging.info(f"ê²€ìƒ‰ ì¡°ê±´ - ìœ„ì¹˜: {location}, ì¹´í…Œê³ ë¦¬: {category}, top_k: {top_k}")
    
    try:
        # 1. ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ (ë” ë§ì€ í›„ë³´êµ° ê²€ìƒ‰)
        category_emb = embed_text(category)
        category_query = np.expand_dims(category_emb, axis=0).astype(np.float32)
        faiss.normalize_L2(category_query)
        category_distances, category_indices = index_category.search(category_query, top_k * 4)
        
        # ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„ê°€ ë†’ì€ í•­ëª©ë§Œ í•„í„°ë§ (ì„ê³„ê°’: 0.7)
        category_threshold = 0.7
        valid_category_mask = (1 - category_distances[0]) >= category_threshold
        category_candidates = category_indices[0][valid_category_mask]
        
        if len(category_candidates) == 0:
            logging.warning(f"ì¹´í…Œê³ ë¦¬ '{category}'ì™€ ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None
            
        logging.info(f"ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(category_candidates)}")
        
        # 2. ìœ„ì¹˜ ê²€ìƒ‰
        location_emb = embed_text(location)
        location_query = np.expand_dims(location_emb, axis=0).astype(np.float32)
        faiss.normalize_L2(location_query)
        location_distances, location_indices = index_location.search(location_query, top_k * 4)
        
        # ìœ„ì¹˜ ìœ ì‚¬ë„ê°€ ë†’ì€ í•­ëª©ë§Œ í•„í„°ë§ (ì„ê³„ê°’: 0.7)
        location_threshold = 0.7
        valid_location_mask = (1 - location_distances[0]) >= location_threshold
        location_candidates = location_indices[0][valid_location_mask]
        
        if len(location_candidates) == 0:
            logging.warning(f"ìœ„ì¹˜ '{location}'ì™€ ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None
            
        logging.info(f"ìœ„ì¹˜ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(location_candidates)}")
        
        # 3. êµì§‘í•© ì°¾ê¸°
        common_indices = np.intersect1d(category_candidates, location_candidates)
        
        if len(common_indices) == 0:
            logging.warning("ìœ„ì¹˜ì™€ ì¹´í…Œê³ ë¦¬ ì¡°ê±´ì„ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None
            
        logging.info(f"êµì§‘í•© ê²°ê³¼ ìˆ˜: {len(common_indices)}")
        
        # 4. ìµœì¢… ê²°ê³¼ ì •ë ¬ (ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„ ê¸°ì¤€)
        final_results = []
        for idx in common_indices:
            cat_sim = 1 - category_distances[0][np.where(category_indices[0] == idx)[0][0]]
            loc_sim = 1 - location_distances[0][np.where(location_indices[0] == idx)[0][0]]
            final_results.append((idx, cat_sim, loc_sim))
        
        # ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„ë¡œ ì •ë ¬
        final_results.sort(key=lambda x: x[1], reverse=True)
        final_results = final_results[:top_k]
        
        # ê²°ê³¼ ë¡œê¹…
        logging.info(f"ìµœì¢… ê²€ìƒ‰ ê²°ê³¼:")
        for idx, cat_sim, loc_sim in final_results:
            logging.info(f"- {df.iloc[idx]['ê°€ë§¹ì ëª…']}")
            logging.info(f"  ì¹´í…Œê³ ë¦¬: {df.iloc[idx]['Categories']}")
            logging.info(f"  ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„: {cat_sim:.4f}")
            logging.info(f"  ìœ„ì¹˜ ìœ ì‚¬ë„: {loc_sim:.4f}")
        
        final_indices = [r[0] for r in final_results]
        final_scores = [r[1] for r in final_results]  # ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„ë¥¼ ìµœì¢… ì ìˆ˜ë¡œ ì‚¬ìš©
        
        return np.array(final_scores), np.array(final_indices)
        
    except Exception as e:
        logging.error(f"FAISS ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        raise

def get_coords_from_address(address):
    url = "https://dapi.kakao.com/v2/local/search/address.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    params = {"query": address}
    try:
        logging.info(f"ì¹´ì¹´ì˜¤ API ìš”ì²­ - ì£¼ì†Œ: {address}")
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        result = response.json()
        logging.info(f"ì¹´ì¹´ì˜¤ API ì‘ë‹µ: {result}")
        
        if result['documents']:
            x = float(result['documents'][0]['x'])
            y = float(result['documents'][0]['y'])
            coords = {'lat': y, 'lng': x}
            logging.info(f"ì¢Œí‘œ ë³€í™˜ ì„±ê³µ: {coords}")
            return coords
    except Exception as e:
        logging.error(f"[ERROR] ì¢Œí‘œ ë³€í™˜ ì‹¤íŒ¨: {e}", exc_info=True)
    return None

def get_coordinates(address):
    """ì¹´ì¹´ì˜¤ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì†Œì˜ ì¢Œí‘œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        url = 'https://dapi.kakao.com/v2/local/search/address.json'
        headers = {'Authorization': f'KakaoAK {KAKAO_API_KEY}'}
        params = {'query': address}
        
        response = requests.get(url, headers=headers, params=params)
        if response.status_code == 200:
            result = response.json()
            if result['documents']:
                x = float(result['documents'][0]['x'])  # ê²½ë„
                y = float(result['documents'][0]['y'])  # ìœ„ë„
                return y, x
        return None, None
    except Exception as e:
        logging.error(f"ì¢Œí‘œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None, None

def display_map(df):
    try:
        # ìƒìœ„ 3ê°œ ê²°ê³¼ë§Œ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
        df = df.head(3)
        logging.info(f"ì§€ë„ ìƒì„± ì‹œì‘ - ìµœì¢… ì¶”ì²œ ì‹ë‹¹ ìˆ˜: {len(df)}")
        
        # ì£¼ì†Œë¥¼ ì¢Œí‘œë¡œ ë³€í™˜
        valid_data = []
        for idx, row in df.iterrows():
            coords = get_coords_from_address(row['ì£¼ì†Œ'])
            if coords:
                # ìœ ì‚¬ë„ ì ìˆ˜ í¬ë§·íŒ…ì„ ìœ„í•œ ì²˜ë¦¬
                similarity_score = row.get('similarity_score', 'N/A')
                if isinstance(similarity_score, float):
                    similarity_score = f"{similarity_score:.4f}"
                
                valid_data.append({
                    'lat': coords['lat'],
                    'lng': coords['lng'],
                    'name': row['ê°€ë§¹ì ëª…'],
                    'address': row['ì£¼ì†Œ'],
                    'category': row['Categories'],
                    'similarity_score': similarity_score,
                    'local_usage': row['í˜„ì§€ì¸ì´ìš©ê±´ìˆ˜ë¹„ì¤‘']
                })
                logging.info(f"ì‹ë‹¹ {idx+1} ì¢Œí‘œ ë³€í™˜ ì„±ê³µ: {row['ê°€ë§¹ì ëª…']} - {coords}")
        
        if not valid_data:
            logging.error("ì¶”ì²œëœ ì‹ë‹¹ì˜ ìœ íš¨í•œ ì¢Œí‘œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        # ì¤‘ì‹¬ì  ê³„ì‚°
        center_lat = sum(data['lat'] for data in valid_data) / len(valid_data)
        center_lng = sum(data['lng'] for data in valid_data) / len(valid_data)
        
        # ì§€ë„ ìƒì„±
        m = folium.Map(
            location=[center_lat, center_lng],
            zoom_start=13,
            width='100%',
            height='100%'
        )
        
        # ë§ˆì»¤ ì¶”ê°€ - ìƒì„¸ ì •ë³´ í¬í•¨
        for data in valid_data:
            popup_content = f"""
            <div style='width: 200px'>
                <h4>{data['name']}</h4>
                <p>ì£¼ì†Œ: {data['address']}</p>
                <p>ì—…ì¢…: {data['category']}</p>
                <p>ìœ ì‚¬ë„: {data['similarity_score']}</p>
                <p>í˜„ì§€ì¸ ì´ìš©: {data['local_usage']:.1f}%</p>
            </div>
            """
            
            folium.Marker(
                location=[data['lat'], data['lng']],
                popup=folium.Popup(popup_content, max_width=300),
                tooltip=data['name'],
                icon=folium.Icon(color='red', icon='info-sign')
            ).add_to(m)
        
        logging.info(f"ì§€ë„ ìƒì„± ì™„ë£Œ - í‘œì‹œëœ ì‹ë‹¹ ìˆ˜: {len(valid_data)}")
        return m
        
    except Exception as e:
        logging.error(f"ì§€ë„ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        return None

# 3. ì‘ë‹µ ìƒì„± í•¨ìˆ˜ (ì¶”ì²œ ë©”ì‹œ ìƒì„±)
def generate_response(prompt, filtered_df, time, local_choice):
    try:
        logging.info(f"\n{'='*50}\nì‘ë‹µ ìƒì„± ì‹œì‘\n{'='*50}")
        
        # ìƒìœ„ 3ê°œ ì‹ë‹¹ ì •ë³´ ì¤€ë¹„
        top_3_restaurants = filtered_df.head(3)
        restaurant_details = []
        
        for _, row in top_3_restaurants.iterrows():
            usage_level = row['ì´ìš©ê±´ìˆ˜êµ¬ê°„']
            local_usage = row['í˜„ì§€ì¸ì´ìš©ê±´ìˆ˜ë¹„ì¤‘']
            time_info = ""
            if time:
                time_usage = row[f'{time}ì´ìš©ê±´ìˆ˜ë¹„ì¤‘']
                time_info = f", {time} ì´ìš© ë¹„ì¤‘: {time_usage:.1f}%"
            
            detail = {
                'ì´ë¦„': row['ê°€ë§¹ì ëª…'],
                'ì£¼ì†Œ': row['ì£¼ì†Œ'],
                'ì—…ì¢…': row['Categories'],
                'ì´ìš©ê±´ìˆ˜êµ¬ê°„': usage_level,
                'í˜„ì§€ì¸ì´ìš©ë¹„ì¤‘': f"{local_usage:.1f}%",
                'ì‹œê°„ëŒ€ì •ë³´': time_info
            }
            restaurant_details.append(detail)
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        full_prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: {prompt}

ì¶”ì²œ ì‹ë‹¹ ì •ë³´:
{'-'*50}
"""
        for i, detail in enumerate(restaurant_details, 1):
            full_prompt += f"""
{i}. {detail['ì´ë¦„']}
- ì£¼ì†Œ: {detail['ì£¼ì†Œ']}
- ì—…ì¢…: {detail['ì—…ì¢…']}
- ì´ìš©ê±´ìˆ˜: {detail['ì´ìš©ê±´ìˆ˜êµ¬ê°„']} (5_75~90%ëŠ” ìƒìœ„ 10~25%, 6_90~100%ëŠ” ìƒìœ„ 10% êµ¬ê°„)
- í˜„ì§€ì¸ ì´ìš© ë¹„ì¤‘: {detail['í˜„ì§€ì¸ì´ìš©ë¹„ì¤‘']}{detail['ì‹œê°„ëŒ€ì •ë³´']}
"""

        full_prompt += f"""
{'-'*50}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. 
- ì´ìš©ê±´ìˆ˜ êµ¬ê°„ì„ ëª…í™•íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”
- í˜„ì§€ì¸ ì´ìš© ë¹„ì¤‘ë„ ì–¸ê¸‰í•´ì£¼ì„¸ìš”
- ê° ì‹ë‹¹ì˜ íŠ¹ì§•ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”
- ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì–´íˆ¬ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”
"""

        logging.info(f"\nLLM ì…ë ¥ í”„ë¡¬í”„íŠ¸:\n{full_prompt}")
        response = model.generate_content(full_prompt)
        logging.info(f"\nLLM ìƒì„± ì‘ë‹µ:\n{response.text}")
        
        return response.text
        
    except Exception as e:
        logging.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        return "ì¶”ì²œì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

def filter_data_by_criteria(df, location, category, time_periods=None, conditions=None):
    try:
        logging.info(f"\n{'='*50}\ní•„í„°ë§ í”„ë¡œì„¸ìŠ¤ ì‹œì‘\n{'='*50}")
        logging.info(f"ì…ë ¥ íŒŒë¼ë¯¸í„°:\n- ìœ„ì¹˜: {location}\n- ì¹´í…Œê³ ë¦¬: {category}\n- ì‹œê°„ëŒ€: {time_periods}\n- ì¡°ê±´: {conditions}")
        logging.info(f"ì´ˆê¸° ë°ì´í„° í¬ê¸°: {len(df)} í–‰")
        
        filtered_df = df.copy()
        
        # 1. FAISS ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰
        logging.info("1. FAISS ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ ì‹œì‘")
        
        # ë¬¸ìì—´ì„ numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
        def parse_embedding(embedding_str):
            # ë¬¸ìì—´ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
            numbers = [float(num) for num in embedding_str.strip('[]').split()]
            return np.array(numbers)

        try:
            # CSV íŒŒì¼ ì½ê¸° ì‹œ ì¸ì½”ë”© ì„¤ì •
            if 'ì¥ì†Œ_ì„ë² ë”©' not in filtered_df.columns:
                embedding_column = [col for col in filtered_df.columns if 'ì„ë² ë”©' in col][0]
                filtered_df = filtered_df.rename(columns={embedding_column: 'ì¥ì†Œ_ì„ë² ë”©'})
            
            # ë°ì´í„°í”„ë ˆì„ì˜ ì„ë² ë”©ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
            embeddings = []
            for emb_str in filtered_df['ì¥ì†Œ_ì„ë² ë”©'].values:
                try:
                    numbers = [float(num) for num in emb_str.strip('[]').split()]
                    embeddings.append(numbers)
                except Exception as e:
                    logging.error(f"ê°œë³„ ì„ë² ë”© ë³€í™˜ ì˜¤ë¥˜: {e}")
                    embeddings.append(np.zeros(768))
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ê³  float32 íƒ€ì…ìœ¼ë¡œ ëª…ì‹œì  ë³€í™˜
            embeddings = np.array(embeddings, dtype=np.float32)
            logging.info(f"ì„ë² ë”© ë°°ì—´ shape: {embeddings.shape}")

            # FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ë²¡í„° ì¶”ê°€
            dimension = embeddings.shape[1]
            index = faiss.IndexFlatIP(dimension)
            
            # L2 ì •ê·œí™” ì „ì— ë³µì‚¬ë³¸ ìƒì„±
            embeddings_normalized = embeddings.copy()
            faiss.normalize_L2(embeddings_normalized)
            
            # ì •ê·œí™”ëœ ë²¡í„° ì¶”ê°€
            index.add(embeddings_normalized)

            # ì¿¼ë¦¬ ì„ë² ë”© ì¤€ë¹„
            location_mask = filtered_df['íŒŒì‹±ëœ_ì¥ì†Œ'].str.contains(location, na=False)
            if location_mask.any():
                location_emb = parse_embedding(filtered_df[location_mask].iloc[0]['ì¥ì†Œ_ì„ë² ë”©'])
                query_emb = location_emb.reshape(1, -1).astype(np.float32)
                
                # ì¿¼ë¦¬ ë²¡í„° ì •ê·œí™”
                query_normalized = query_emb.copy()
                faiss.normalize_L2(query_normalized)

                # ìœ ì‚¬ë„ ê²€ìƒ‰
                k = min(1000, len(filtered_df))
                similarities, indices = index.search(query_normalized, k)

                logging.info(f"FAISS ê²€ìƒ‰ ê²°ê³¼ - ìµœëŒ€ ìœ ì‚¬ë„: {similarities[0][0]:.4f}, ìµœì†Œ ìœ ì‚¬ë„: {similarities[0][-1]:.4f}")

                # ìœ ì‚¬ë„ ì„ê³„ê°’ ì ìš©
                threshold = 0.3
                valid_indices = indices[0][similarities[0] > threshold]
                filtered_df = filtered_df.iloc[valid_indices].copy()
                filtered_df['similarity_score'] = similarities[0][similarities[0] > threshold]

                logging.info(f"ì„ë² ë”© ê¸°ë°˜ í•„í„°ë§ í›„ ë°ì´í„° ìˆ˜: {len(filtered_df)}")
            else:
                logging.warning(f"ìœ„ì¹˜ '{location}'ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()

        except Exception as e:
            logging.error(f"ì„ë² ë”© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return pd.DataFrame()

        # 2. ì‹œê°„ëŒ€ í•„í„°ë§
        if time_periods:
            logging.info("2. ì‹œê°„ëŒ€ í•„í„° ì‹œì‘")
            time_conditions = []
            for time in time_periods:
                if time == 'ì•„ì¹¨':
                    time_conditions.append(filtered_df['5ì‹œ11ì‹œì´ìš©ê±´ìˆ˜ë¹„ì¤‘'] > 0)
                elif time == 'ì ì‹¬':
                    time_conditions.append(filtered_df['12ì‹œ13ì‹œì´ìš©ê±´ìˆ˜ë¹„ì¤‘'] > 0)
                elif time == 'ì €ë…':
                    time_conditions.append(filtered_df['18ì‹œ22ì‹œì´ìš©ê±´ìˆ˜ë¹„ì¤‘'] > 0)
                elif time == 'ë°¤':
                    time_conditions.append(filtered_df['23ì‹œ4ì‹œì´ìš©ê±´ìˆ˜ë¹„ì¤‘'] > 0)
            
            if time_conditions:
                combined_condition = time_conditions[0]
                for condition in time_conditions[1:]:
                    combined_condition |= condition
                filtered_df = filtered_df[combined_condition]
                logging.info(f"ì‹œê°„ëŒ€ í•„í„°ë§ í›„ ë°ì´í„° ìˆ˜: {len(filtered_df)}")
        
        # 3. ì¡°ê±´ í•„í„°ë§
        if conditions:
            logging.info("3. ì¶”ê°€ ì¡°ê±´ í•„í„°ë§ ì‹œì‘")
            for condition in conditions:
                before_count = len(filtered_df)
                
                if 'ì´ìš©ê±´ìˆ˜ ìƒìœ„' in condition.lower():
                    if '10%' in condition:
                        filtered_df = filtered_df[filtered_df['ì´ìš©ê±´ìˆ˜êµ¬ê°„'].isin(['5_75~90%', '6_90~100%'])]
                    elif '20%' in condition:
                        filtered_df = filtered_df[filtered_df['ì´ìš©ê±´ìˆ˜êµ¬ê°„'].isin(['4_50~75%', '5_75~90%', '6_90~100%'])]
                    logging.info(f"ì´ìš©ê±´ìˆ˜ í•„ë§ - {condition} ì ìš© í›„: {len(filtered_df)}ê°œ")
                
                elif 'í˜„ì§€ì¸' in condition.lower():
                    filtered_df = filtered_df.sort_values('í˜„ì§€ì¸ì´ìš©ê±´ìˆ˜ë¹„ì¤‘', ascending=False)
                    logging.info(f"í˜„ì§€ì¸ ì´ìš© ë¹„ì¤‘ ì •ë ¬ - ìµœëŒ€ê°’: {filtered_df['í˜„ì§€ì¸ì´ìš©ê±´ìˆ˜ë¹„ì¤‘'].max():.2f}")
                
                elif 'ìš”ì¼' in condition.lower():
                    day_columns = {
                        'ì›”ìš”ì¼': 'ì›”ìš”ì¼ì´ìš©ê±´ìˆ˜ë¹„ì¤‘',
                        'í™”ìš”ì¼': 'í™”ìš”ì¼ì´ìš©ê±´ìˆ˜ë¹„ì¤‘',
                        'ìˆ˜ìš”ì¼': 'ìˆ˜ìš”ì¼ì´ìš©ê±´ìˆ˜ë¹„ì¤‘',
                        'ëª©ìš”ì¼': 'ëª©ìš”ì¼ì´ìš©ê±´ìˆ˜ë¹„ì¤‘',
                        'ê¸ˆìš”ì¼': 'ê¸ˆìš”ì¼ì´ìš©ê±´ìˆ˜ë¹„ì¤‘',
                        'í† ìš”ì¼': 'í† ìš”ì¼ì´ìš©ê±´ìˆ˜ë¹„ì¤‘',
                        'ì¼ìš”ì¼': 'ì¼ìš”ì¼ì´ìš©ê±´ìˆ˜ë¹„ì¤‘'
                    }
                    
                    for day, column in day_columns.items():
                        if day in condition:
                            filtered_df = filtered_df.sort_values(column, ascending=False)
                            logging.info(f"{day} ì´ìš© ë¹„ì¤‘ - ìµœëŒ€ê°’: {filtered_df[column].max():.2f}")
                
                elif 'ëŒ€' in condition:
                    age_columns = {
                        '20ëŒ€': 'ìµœê·¼12ê°œì›”20ëŒ€ì´í•˜íšŒì›ìˆ˜ë¹„ì¤‘',
                        '30ëŒ€': 'ìµœê·¼12ê°œì›”30ëŒ€íšŒì›ìˆ˜ë¹„ì¤‘',
                        '40ëŒ€': 'ìµœê·¼12ê°œì›”40ëŒ€íšŒì›ìˆ˜ë¹„ì¤‘',
                        '50ëŒ€': 'ìµœê·¼12ê°œì›”50ëŒ€íšŒì›ìˆ˜ë¹„ì¤‘',
                        '60ëŒ€': 'ìµœê·¼12ê°œì›”60ëŒ€ì´ìƒíšŒì›ìˆ˜ë¹„ì¤‘'
                    }
                    
                    for age, column in age_columns.items():
                        if age in condition:
                            if 'ìƒ' in condition:
                                threshold = float(condition.split('%')[0].split('ì´ìƒ')[0].strip())
                                filtered_df = filtered_df[filtered_df[column] >= threshold]
                            logging.info(f"{age} í•„í„°ë§ - {condition} ì ìš© í›„: {len(filtered_df)}ê°œ")
                
                logging.info(f"ì¡°ê±´ '{condition}' ì ìš© í›„ ë°ì´í„° ìˆ˜ ë³€í™”: {before_count} -> {len(filtered_df)}")
        
        # ê° í•„í„°ë§ ë‹¨ê³„ë§ˆë‹¤ ìƒì„¸ ë¡œê¹… ì¶”ê°€
        if time_periods:
            for time in time_periods:
                before_count = len(filtered_df)
                # ... ê¸°ì¡´ ì‹œê°„ëŒ€ í•„í„°ë§ ë¡œì§ ...
                after_count = len(filtered_df)
                logging.info(f"ì‹œê°„ëŒ€ '{time}' í•„í„°ë§ ê²°ê³¼: {before_count} -> {after_count} í–‰")
        
        # ì¡°ê±´ í•„í„°ë§ì— ìƒì„¸ ë¡œê¹… ì¶”ê°€
        if conditions:
            for condition in conditions:
                before_count = len(filtered_df)
                # ... ê¸°ì¡´ ì¡°ê±´ í•„í„°ë§ ë¡œì§ ...
                after_count = len(filtered_df)
                logging.info(f"ì¡°ê±´ '{condition}' í•„í„°ë§ ê²°ê³¼: {before_count} -> {after_count} í–‰")
                
                # ì¡°ê±´ë³„ ìƒì„¸ ì •ë³´ ë¡œê¹…
                if 'ì´ìš©ê±´ìˆ˜ ìƒìœ„' in condition.lower():
                    logging.info(f"ì´ìš©ê±´ìˆ˜ êµ¬ê°„ ë¶„í¬:\n{filtered_df['ì´ìš©ê±´ìˆ˜êµ¬ê°„'].value_counts()}")
                elif 'í˜„ì§€ì¸' in condition.lower():
                    logging.info(f"í˜„ì§€ ì´ìš© ë¹„ì¤‘ í†µê³„:\n- í‰ê· : {filtered_df['í˜„ì§€ì¸ì´ìš©ê±´ìˆ˜ë¹„ì¤‘'].mean():.2f}\n- ìµœëŒ€: {filtered_df['í˜„ì§€ì¸ì´ìš©ê±´ìˆ˜ë¹„ì¤‘'].max():.2f}")
        
        # ìµœì¢… ê²°ê³¼ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ ë¡œê¹…
        logging.info(f"\n{'='*50}\nìµœì¢… ê²°ê³¼ ìƒì„¸ ì •ë³´\n{'='*50}")
        for idx, row in filtered_df.head().iterrows():
            logging.info(f"\nì‹ë‹¹ {idx+1} ìƒì„¸ì •ë³´:")
            logging.info(f"- ì´ë¦„: {row['ê°€ë§¹ì ëª…']}")
            logging.info(f"- ì£¼ì†Œ: {row['ì£¼ì†Œ']}")
            logging.info(f"- ì¹´í…Œê³ ë¦¬: {row['Categories']}")
            logging.info(f"- ìœ ì‚¬ë„ ì ìˆ˜: {row.get('similarity_score', 'N/A'):.4f}")
            logging.info(f"- í˜„ì§€ì¸ ì´ìš© ë¹„ì¤‘: {row['í˜„ì§€ì¸ì´ìš©ê±´ìˆ˜ë¹„ì¤‘']:.2f}%")
            if time_periods:
                for time in time_periods:
                    if time == 'ì•„ì¹¨':
                        logging.info(f"- ì•„ì¹¨ ì´ìš© ë¹„ì¤‘: {row['5ì‹œ11ì‹œì´ìš©ê±´ìˆ˜ë¹„ì¤‘']:.2f}%")
                    elif time == 'ì ì‹¬':
                        logging.info(f"- ì ì‹¬ ì´ìš© ë¹„ì¤‘: {row['12ì‹œ13ì‹œì´ìš©ê±´ìˆ˜ë¹„ì¤‘']:.2f}%")
                    elif time == 'ì €ë…':
                        logging.info(f"- ì €ë… ì´ìš© ë¹„ì¤‘: {row['18ì‹œ22ì‹œì´ìš©ê±´ìˆ˜ë¹„ì¤‘']:.2f}%")
                    elif time == 'ë°¤':
                        logging.info(f"- ì‹¬ì•¼ ì´ìš© ë¹„ì¤‘: {row['23ì‹œ4ì‹œì´ìš©ê±´ìˆ˜ë¹„ì¤‘']:.2f}%")
        
        return filtered_df

    except Exception as e:
        logging.error(f"í•„í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        return pd.DataFrame()

# í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± í•¨ìˆ˜
def get_embedding(text):
    try:
        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
        if torch.cuda.is_available():
            inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = embedding_model(**inputs)
        
        # [CLS] í† í°ì˜ ì„ë² ë”© ì‚¬ìš©
        embedding = outputs.last_hidden_state[0][0].cpu().numpy()
        return embedding
        
    except Exception as e:
        logging.error(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return np.zeros(768)  # ê¸°ë³¸ ì„ë² ë”© ì°¨ì›

def parse_user_input_to_json(user_input, retries=3):
    logging.info(f"ì‚¬ìš©ì ì…ë ¥ íŒŒì‹± ì‹œì‘: {user_input}")
    
    prompt = f"""
    ì œì£¼ë„ ë§›ì§‘ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì…ë ¥ì„ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.

    ë‹¤ìŒ ë‹¨ê³„ì— ë”°ë¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

    1. ì¥ì†Œ ì •ë³´ ì¶”ì¶œ
    - ì œì£¼ì‹œ/ì„œê·€í¬ì‹œ êµ¬ë¶„
    - ì/ë©´/ë™ ë‹¨ìœ„ê¹Œì§€ í¬í•¨
    - ì˜ˆ: "ì œì£¼ì‹œ ë…¸í˜•ë™", "ì„œê·€í¬ì‹œ ìƒ‰ë‹¬ë™"

    2. ì—…ì¢… ë¶„ë¥˜
    - ì£¼ìš” ì¹´í…Œê³ ë¦¬: "ìœ¡ë¥˜,ê³ ê¸°ìš”ë¦¬", "í•´ë¬¼,ìƒì„ ìš”ë¦¬", "ê°€ì •ì‹", "ë‹¨í’ˆìš”ë¦¬ ì „ë¬¸" ë“±
    - êµ¬ì²´ì  ì—…ì¢…ìœ¼ë¡œ ë§¤í•‘

    3. ì‹œê°„ëŒ€ ë¶„ì„
    - ì•„ì¹¨(5-11ì‹œ), ì ì‹¬(12-13ì‹œ), ì €ë…(17-21ì‹œ), ë°¤(22-23ì‹œ)
    - í•´ë‹¹í•˜ëŠ” ì‹œê°„ëŒ€ ë°°ì—´ë¡œ í‘œí˜„

    4. ì¡°ê±´ ë¶„ì„ (ì•„ë˜ ì¡°ê±´ë§Œ ì‚¬ìš© ê°€ëŠ¥)
    - ì´ìš©ê±´ìˆ˜: "ì´ìš©ê±´ìˆ˜ ìƒìœ„ 10%", "ì´ìš©ê±´ìˆ˜ ìƒìœ„ 20%"
    - ì—°ë ¹ëŒ€: "20ëŒ€ ì´ìš© ë¹„ì¤‘ 30% ì´ìƒ", "30ëŒ€ ì´ìš© ë¹„ì¤‘ 30% ì´ìƒ" ë“±
    - ìš”ì¼: "ì›”ìš”ì¼ ì´ìš© ë¹„ì¤‘ ê°€ì¥ ë†’ìŒ", "í™”ìš”ì¼ ì´ìš© ë¹„ì¤‘ ê°€ì¥ ë†’ìŒ" ë“±
    - í˜„ì§€ì¸: "í˜„ì§€ì¸ ì´ìš© ë¹„ì¤‘ ê°€ì¥ ë†’ìŒ"

    â€» ë¶€ëª¨ë‹˜/ê°€ì¡± ê´€ë ¨ í‚¤ì›Œë“œê°€ ë‚˜ì˜¤ë©´ ë°˜ë“œì‹œ "50ëŒ€ ì´ìš© ë¹„ì¤‘ 30% ì´ìƒ"ìœ¼ë¡œ ë³€í™˜

    ì¶œë ¥ í˜•ì‹:
    {{
        "ì¥ì†Œ": "ì§€ì—­ëª…",
        "Categories": "ì—…ì¢…",
        "ì‹œê°„ëŒ€": ["ì•„ì¹¨", "ì ì‹¬", "ì €ë…", "ë°¤"],
        "ì¡°ê±´": ["ì¡°ê±´1", "ì¡°ê±´2", ...]
    }}

    ì˜ˆì‹œ ì…ë ¥/ì¶œë ¥:
    1. ì…ë ¥: "ì œì£¼ì‹œ ë…¸í˜•ë™ì— ìˆëŠ” ë‹¨í’ˆìš”ë¦¬ ì „ë¬¸ì  ì¤‘ ì´ìš©ê±´ìˆ˜ê°€ ìƒìœ„ 10%ì— ì†í•˜ê³  í˜„ì§€ì¸ ì´ìš© ë¹„ì¤‘ì´ ê°€ì¥ ë†’ì€ ê³³ì€?"
    â†’ {{
        "ì¥ì†Œ": "ì œì£¼ì‹œ ë…¸í˜•ë™",
        "Categories": "ë‹¨í’ˆìš”ë¦¬ ì „ë¬¸",
        "ì‹œê°„ëŒ€": [],
        "ì¡°ê±´": ["ì´ìš©ê±´ìˆ˜ ìƒìœ„ 10%", "í˜„ì§€ì¸ ì´ìš© ë¹„ì¤‘ ê°€ì¥ ë†’ìŒ"]
    }}

    2. ì…ë ¥: "ë¶€ëª¨ë‹˜ê³¼ í•¨ê»˜ ê°ˆ ì„œê·€í¬ì‹œ ìƒ‰ë‹¬ë™ì˜ ë‹¨í’ˆìš”ë¦¬ ì „ë¬¸ì  ì¶”ì²œí•´ì¤˜"
    â†’ {{
        "ì¥ì†Œ": "ì„œê·€í¬ì‹œ ìƒ‰ë‹¬ë™",
        "Categories": "ë‹¨í’ˆìš”ë¦¬ ì „ë¬¸",
        "ì‹œê°„ëŒ€": [],
        "ì¡°ê±´": ["50ëŒ€ ì´ìš© ë¹„ì¤‘ 30% ì´ìƒ"]
    }}

    í˜„ì¬ ì…ë ¥: {user_input}
    ìœ„ ì˜ˆì‹œë“¤ì„ ì°¸ê³ í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
    """
    
    for attempt in range(retries):
        try:
            response = model.generate_content(prompt)
            json_str = response.text.strip().replace("```json", "").replace("```", "").strip()
            logging.info(f"LLM ì‘ë‹µ: {json_str}")
            
            parsed_input = eval(json_str)
            if "ì¥ì†Œ" in parsed_input and "Categories" in parsed_input:
                logging.info(f"íŒŒì‹± ì„±ê³µ: {parsed_input}")
                return parsed_input
        except Exception as e:
            logging.error(f"JSON íŒŒì‹± ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {str(e)}")
            if attempt == retries - 1:
                raise
    return None

# ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™” í•¨ìˆ˜
def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë–¤ ì‹ë‹¹ì„ ì°¾ìœ¼ì‹œë‚˜ìš”?"}]
    logging.info("ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”")

# Streamlit UI ì„¤ì •
st.title("ğŸŠ ì œì£¼ ë§›ì§‘ ì¶”ì²œ ì±—ë´‡")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë¶€ë¶„
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë–¤ ì‹ë‹¹ì„ ì°¾ìœ¼ì‹œë‚˜ìš”?"}]
    
if "map_data" not in st.session_state:
    st.session_state.map_data = None

# ì‚¬ì´ë“œë°”ì— ì´ˆê¸°í™” ë²„íŠ¼ ì¶”ê°€
st.sidebar.button('ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì§€ìš°ê¸°', on_click=clear_chat_history)

# ì´ì „ ë©”ì‹œì§€ë“¤ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë¶€ë¶„
if prompt := st.chat_input("ì˜ˆ: ì• ì›”ì˜ ì €ë… ìƒìœ„ 10% ê³ ê¸°ì§‘ ì¶”ì²œí•´ì¤˜"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    try:
        parsed_input = parse_user_input_to_json(prompt)
        if parsed_input:
            location = parsed_input["ì¥ì†Œ"]
            category = parsed_input["Categories"]
            time = parsed_input.get("ì‹œê°„ëŒ€", [])
            conditions = parsed_input.get("ì¡°ê±´", [])

            result_df = filter_data_by_criteria(df, location, category, time_periods=time, conditions=conditions)

            # ì‘ë‹µ ìƒì„±
            with st.chat_message("assistant"):
                generated_response = generate_response(prompt, result_df, ", ".join(time), "ë§›ì§‘")
                st.write(generated_response)
                st.session_state.messages.append({"role": "assistant", "content": generated_response})

                # ì§€ë„ í‘œì‹œ
                if result_df is not None and not result_df.empty:
                    with st.spinner('ì§€ë„ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘...'):
                        m = display_map(result_df)
                        if m:
                            st.write("## ì¶”ì²œëœ ì‹ë‹¹ ìœ„ì¹˜")
                            st_folium(m, width=700, height=500)
                            st.session_state.map_data = m  # ì§€ë„ ë°ì´í„° ì €ì¥
                        else:
                            st.warning("ì£„ì†¡í•©ë‹ˆë‹¤. ì§€ë„ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        else:
            with st.chat_message("assistant"):
                error_message = "ì¥ì†Œì™€ ì—…ì¢…ì„ ì •í™•íˆ ì…ë ¥í•´ì£¼ì„¸ìš”."
                st.warning(error_message)
                st.session_state.messages.append({"role": "assistant", "content": error_message})
    except Exception as e:
        with st.chat_message("assistant"):
            error_message = "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            st.error(error_message)
            st.session_state.messages.append({"role": "assistant", "content": error_message})
        logging.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)

# ì €ì¥ëœ ì§€ë„ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í‘œì‹œ
if st.session_state.map_data is not None:
    st.write("## ì¶”ì²œëœ ì‹ë‹¹ ìœ„ì¹˜")
    st_folium(st.session_state.map_data, width=700, height=500)




















