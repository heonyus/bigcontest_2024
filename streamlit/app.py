import streamlit as st
# Streamlit í˜ì´ì§€ ì„¤ì •ì€ ê°€ì¥ ë¨¼ì € í˜¸ì¶œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
st.set_page_config(page_title="ğŸŠ ì œì£¼ ë§›ì§‘ ì¶”ì²œ ì±—ë´‡", page_icon="ğŸŠ", layout="wide")

import os
os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'
import numpy as np
import pandas as pd
import torch
import faiss
import requests
import folium
from streamlit_folium import st_folium
from transformers import AutoTokenizer, AutoModel
import google.generativeai as genai
from tqdm import tqdm
import logging
import sys
import time as time_module  # time ëª¨ë“ˆì— ë³„ì¹­ ë¶€ì—¬
from datetime import datetime
import ast
import re
from geopy.distance import geodesic
import math  # íŒŒì¼ ìƒë‹¨ì˜ ë‹¤ë¥¸ importë¬¸ë“¤ê³¼ í•¨ê»˜ ì¶”ê°€

# ë¡œê¹… ë ˆë²¨ ì„¤ì •ì„ DEBUGë¡œ ë³€ê²½
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),  # stdoutìœ¼ë¡œ ë³€ê²½
        logging.FileHandler("app_log.log", encoding='utf-8')  # ì¸ì½”ë”© ì§€ì •
    ]
)

# 1. ê¸°ë³¸ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
KAKAO_API_KEY = st.secrets["KAKAO_API_KEY"]

genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel("gemini-1.5-flash")

# transformers import ìˆ˜ì •
try:
    from transformers import AutoTokenizer, AutoModel
    logging.info("Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    logging.error(f"Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
    st.error("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œì— ì‹¤íŒ¨í–ˆ. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
    st.stop()

# ëª¨ë¸ ë¡œë“œ ë¶€ë¶„ ìˆ˜ì •
try:
    embedding_model_name = "jhgan/ko-sroberta-multitask"
    tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)
    embedding_model = AutoModel.from_pretrained(embedding_model_name)
    
    if torch.cuda.is_available():
        embedding_model = embedding_model.to(device)
        logging.info("ëª¨ë¸ì„ GPUë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    else:
        logging.info("ëª¨ë¸ì„ CPUë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    logging.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    st.error("ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
    st.stop()

embedding_data_path = './data/JEJU_with_embeddings_donghyeok.csv'

# ì„ë² ë”© ë¬¸ìì—´ì„ numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def str_to_array(embedding_str):
    """ë¬¸ìì—´ì„ numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
    try:
        if isinstance(embedding_str, str):
            # ë¬¸ìì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
            numbers = [float(num) for num in embedding_str.strip('[]').split()]
            return np.array(numbers)
        else:
            # ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°, ì˜ˆë¥¼ ë“¤ì–´ NaN, float ë“±
            logging.error(f"ì„ë² ë”© ë³€í™˜ ì˜¤ë¥˜: {embedding_str}ëŠ” ë¬¸ìì—´ì´ ì•„ë‹™ë‹ˆë‹¤.")
            return np.zeros(768)  # ê¸°ë³¸ í¬ê¸°ì˜ 0 ë°°ì—´ ë°˜í™˜
    except Exception as e:
        logging.error(f"ì„ë² ë”© ë³€í™˜ ì˜¤ë¥˜: {e}")
        return np.zeros(768)  # ê¸°ë³¸ í¬ê¸°ì˜ 0 ë°°ì—´ ë°˜í™˜

# 2. FAISS ì¸ë±ìŠ¤ ìƒì„± í•¨ìˆ˜ (CPU ì „ìš©)
def create_faiss_index(embeddings):
    try:
        dimension = embeddings.shape[1]
        logging.info(f"FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹œì‘ (ì°¨ì›: {dimension})")
        
        # CPU ë²„ì „ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©
        index = faiss.IndexFlatL2(dimension)
        index.add(embeddings)
        
        logging.info("FAISS CPU ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
        return index
    except Exception as e:
        logging.error(f"FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        raise

# ì•± ì‹œì‘ ë¶€ë¶„ì— ì¶”ê°€
@st.cache_resource
def load_models():
    """ëª¨ë¸ ë¡œë“œë¥¼ ìºì‹œí•˜ì—¬ ì¬ì‚¬ìš©"""
    try:
        embedding_model_name = "jhgan/ko-sroberta-multitask"
        tokenizer = AutoTokenizer.from_pretrained(embedding_model_name) 
        embedding_model = AutoModel.from_pretrained(embedding_model_name)
        
        if torch.cuda.is_available():
            embedding_model = embedding_model.to(device)
            logging.info("ëª¨ë¸ì„ GPUë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        else:
            logging.info("ëª¨ë¸ì„ CPUë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            
        return tokenizer, embedding_model
    except Exception as e:
        logging.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.error("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
        st.stop()

# ì „ì—­ ë³€ìˆ˜ë¡œ í‚¤ì›Œë“œ ì„ë² ë”© ì €ì¥
keyword_embeddings_dict = {}
initialized = False

def embed_text(text):
    """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
    try:
        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)
        if torch.cuda.is_available():
            inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = embedding_model(**inputs)
            embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()[0]
        return embedding
    except Exception as e:
        logging.error(f"í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
        return np.zeros(768)

@st.cache_resource
def load_and_process_data():
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ë¥¼ ìºì‹œí•˜ì—¬ ì¬ì‚¬ìš©"""
    try:
        # JEJU_with_embeddings_donghyeok.csv ë¡œë“œ
        logging.info("JEJU_with_embeddings_donghyeok.csv ë¡œë“œ ì‹œì‘")
        df = pd.read_csv('./data/JEJU_with_embeddings_donghyeok.csv', encoding='utf-8-sig')
        df['ê°€ë§¹ì ëª…'] = df['ê°€ë§¹ì ëª…'].str.replace(' ', '')
        
        logging.info(f"ë°ì´í„° í¬ê¸°: {len(df)}")
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        location_embeddings = np.stack(df['ì¥ì†Œ_ì„ë² ë”©'].apply(str_to_array).to_numpy()).astype(np.float32)
        category_embeddings = np.stack(df['ì¹´í…Œê³ ë¦¬_ì„ë² ë”©'].apply(str_to_array).to_numpy()).astype(np.float32)
        
        faiss.normalize_L2(location_embeddings)
        faiss.normalize_L2(category_embeddings)
        
        index_location = faiss.IndexFlatIP(location_embeddings.shape[1])
        index_category = faiss.IndexFlatIP(category_embeddings.shape[1])
        
        index_location.add(location_embeddings)
        index_category.add(category_embeddings)
        
        return df, index_location, index_category
        
    except Exception as e:
        logging.error(f"ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

# ë©”ì¸ ì½”ë“œì—ì„œ ì‚¬ìš©
tokenizer, embedding_model = load_models()
df, index_location, index_category = load_and_process_data()

def search_with_faiss(location, category, top_k=5):
    logging.info(f"=== FAISS ê²€ìƒ‰ ì‹œì‘ ===")
    logging.info(f"ê²€ìƒ‰ ì¡°ê±´ - ìœ„ì¹˜: {location}, ì¹´í…Œê³ ë¦¬: {category}, top_k: {top_k}")
    
    try:
        # 1. ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ (ë” ë§ì€ í›„ë³´êµ° ê²€ìƒ‰)
        category_emb = embed_text(category)
        category_query = np.expand_dims(category_emb, axis=0).astype(np.float32)
        faiss.normalize_L2(category_query)
        category_distances, category_indices = index_category.search(category_query, top_k * 4)
        
        # ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„ê°€ ë†’ì€ í•­ëª©ë§Œ í•„í„°ë§ (ì„ê³„ê°’: 0.7)
        category_threshold = 0.7
        valid_category_mask = (1 - category_distances[0]) >= category_threshold
        category_candidates = category_indices[0][valid_category_mask]
        
        if len(category_candidates) == 0:
            logging.warning(f"ì¹´í…Œê³ ë¦¬ '{category}'ì™€ ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None
            
        logging.info(f"ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(category_candidates)}")
        
        # 2. ìœ„ì¹˜ ê²€ìƒ‰
        location_emb = embed_text(location)
        location_query = np.expand_dims(location_emb, axis=0).astype(np.float32)
        faiss.normalize_L2(location_query)
        location_distances, location_indices = index_location.search(location_query, top_k * 4)
        
        # ìœ„ì¹˜ ìœ ì‚¬ë„ê°€ ë†’ì€ í•­ëª©ë§Œ í•„í„°ë§ (ì„ê³„ê°’: 0.7)
        location_threshold = 0.7
        valid_location_mask = (1 - location_distances[0]) >= location_threshold
        location_candidates = location_indices[0][valid_location_mask]
        
        if len(location_candidates) == 0:
            logging.warning(f"ìœ„ì¹˜ '{location}'ì™€ ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None
            
        logging.info(f"ìœ„ì¹˜ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(location_candidates)}")
        
        # 3. êµì§‘í•© ì°¾ê¸°
        common_indices = np.intersect1d(category_candidates, location_candidates)
        
        if len(common_indices) == 0:
            logging.warning("ìœ„ì¹˜ì™€ ì¹´í…Œê³ ë¦¬ ì¡°ê±´ì„ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None
            
        logging.info(f"êµì§‘í•© ê²°ê³¼ ìˆ˜: {len(common_indices)}")
        
        # 4. ìµœì¢… ê²°ê³¼ ì •ë ¬ (ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„ ê¸°ì¤€)
        final_results = []
        for idx in common_indices:
            cat_sim = 1 - category_distances[0][np.where(category_indices[0] == idx)[0][0]]
            loc_sim = 1 - location_distances[0][np.where(location_indices[0] == idx)[0][0]]
            final_results.append((idx, cat_sim, loc_sim))
        
        # ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„ë¡œ ì •ë ¬
        final_results.sort(key=lambda x: x[1], reverse=True)
        final_results = final_results[:top_k]
        
        # ê²°ê³¼ ë¡œê¹…
        logging.info(f"ìµœì¢… ê²€ìƒ‰ ê²°ê³¼:")
        for idx, cat_sim, loc_sim in final_results:
            logging.info(f"- {df.iloc[idx]['ê°€ë§¹ì ëª…']}")
            logging.info(f"  ì¹´í…Œê³ ë¦¬: {df.iloc[idx]['Categories']}")
            logging.info(f"  ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„: {cat_sim:.4f}")
            logging.info(f"  ìœ„ì¹˜ ìœ ì‚¬ë„: {loc_sim:.4f}")
        
        final_indices = [r[0] for r in final_results]
        final_scores = [r[1] for r in final_results]  # ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„ë¥¼ ìµœì¢… ì ìˆ˜ë¡œ ì‚¬ìš©
        
        return np.array(final_scores), np.array(final_indices)
        
    except Exception as e:
        logging.error(f"FAISS ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        raise

def get_coords_from_address(address):
    url = "https://dapi.kakao.com/v2/local/search/address.json"
    headers = {"Authorization": f"KakaoAK {KAKAO_API_KEY}"}
    params = {"query": address}
    try:
        logging.info(f"ì¹´ì¹´ì˜¤ API ìš”ì²­ - ì£¼ì†Œ: {address}")
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        result = response.json()
        logging.info(f"ì¹´ì¹´ì˜¤ API ì‘ë‹µ: {result}")
        
        if result['documents']:
            x = float(result['documents'][0]['x'])
            y = float(result['documents'][0]['y'])
            coords = {'lat': y, 'lng': x}
            logging.info(f"ì¢Œí‘œ ë³€í™˜ ì„±ê³µ: {coords}")
            return coords
    except Exception as e:
        logging.error(f"[ERROR] ì¢Œí‘œ ë³€í™˜ ì‹¤íŒ¨: {e}", exc_info=True)
    return None

def get_coordinates(address):
    """ì¹´ì¹´ì˜¤ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì†Œì˜ ì¢Œí‘œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        url = 'https://dapi.kakao.com/v2/local/search/address.json'
        headers = {'Authorization': f'KakaoAK {KAKAO_API_KEY}'}
        params = {'query': address}
        
        response = requests.get(url, headers=headers, params=params)
        if response.status_code == 200:
            result = response.json()
            if result['documents']:
                x = float(result['documents'][0]['x'])  # ê²½ë„
                y = float(result['documents'][0]['y'])  # ìœ„ë„
                return y, x
        return None, None
    except Exception as e:
        logging.error(f"ì¢Œí‘œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None, None

def display_map(df, next_df=None):
    try:
        # ìƒìœ„ 5ê°œ ê²°ê³¼ ì‚¬ìš©
        df = df.head(5)
        logging.info(f"ì§€ë„ ìƒì„± ì‹œì‘ - í˜„ì¬ ì¶”ì²œ ì‹ë‹¹ ìˆ˜: {len(df)}")
        
        valid_data = []
        current_coords = []
        
        # í˜„ì¬ ì¶”ì²œ ì¥ì†Œë“¤ì˜ ì¢Œí‘œ ì²˜ë¦¬
        for idx, row in df.iterrows():
            coords = get_coords_from_address(row['ì£¼ì†Œ'])
            if coords:
                current_coords.append((coords['lat'], coords['lng']))
                valid_data.append({
                    'lat': coords['lat'],
                    'lng': coords['lng'],
                    'name': row['ê°€ë§¹ì ëª…'],
                    'address': row['ì£¼ì†Œ'],
                    'category': row['Categories'],
                    'similarity_score': row.get('similarity_score', 'N/A'),
                    'local_usage': row['í˜„ì§€ì¸ì´ìš©ê±´ìˆ˜ë¹„ì¤‘'],
                    'is_current': True
                })
        
        # ë‹¤ìŒ ì‹œê°„ëŒ€ ì¶”ì²œ ì¥ì†Œë“¤ì˜ ì¢Œí‘œ ì²˜ë¦¬
        next_coords = []
        if next_df is not None and not next_df.empty:
            next_df = next_df.head(5)
            logging.info(f"ë‹¤ìŒ ì‹œê°„ëŒ€ ì¶”ì²œ ì‹ë‹¹ ìˆ˜: {len(next_df)}")
            for idx, row in next_df.iterrows():
                coords = get_coords_from_address(row['ì£¼ì†Œ'])
                if coords:
                    next_coords.append((coords['lat'], coords['lng']))
                    valid_data.append({
                        'lat': coords['lat'],
                        'lng': coords['lng'],
                        'name': row['ê°€ë§¹ì ëª…'],
                        'address': row['ì£¼ì†Œ'],
                        'category': row['Categories'],
                        'similarity_score': row.get('similarity_score', 'N/A'),
                        'local_usage': row['í˜„ì§€ì¸ì´ìš©ê±´ìˆ˜ë¹„ì¤‘'],
                        'is_current': False
                    })
        
        if not valid_data:
            logging.error("ìœ íš¨í•œ ì¢Œí‘œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        # ëª¨ë“  ì¢Œí‘œì˜ ì¤‘ì‹¬ì  ê³„ì‚°
        all_lats = [d['lat'] for d in valid_data]
        all_lngs = [d['lng'] for d in valid_data]
        center_lat = sum(all_lats) / len(all_lats)
        center_lng = sum(all_lngs) / len(all_lngs)
        
        # ì§€ë„ ìƒì„±
        m = folium.Map(
            location=[center_lat, center_lng],
            zoom_start=12
        )
        
        # í˜„ì¬ ì¶”ì²œ ê·¸ë£¹ê³¼ ë‹¤ìŒ ì¶”ì²œ ê·¸ë£¹ ìƒì„±
        current_group = folium.FeatureGroup(name="í˜„ì¬ ì¶”ì²œ")
        next_group = folium.FeatureGroup(name="ë‹¤ìŒ ì‹œê°„ëŒ€ ì¶”ì²œ")
        
        # í˜„ì¬ ì¶”ì²œ ê·¸ë£¹ì˜ í´ëŸ¬ìŠ¤í„° ì› ê·¸ë¦¬ê¸°
        if current_coords:
            # í˜„ì¬ ì¶”ì²œ ê·¸ë£¹ì˜ ì¤‘ì‹¬ì  ê³„ì‚°
            current_center_lat = sum(lat for lat, _ in current_coords) / len(current_coords)
            current_center_lng = sum(lng for _, lng in current_coords) / len(current_coords)
            
            # ì¤‘ì‹¬ì ì—ì„œ ê°€ì¥ ë¨¼ ë§ˆì»¤ê¹Œì§€ì˜ ê±°ë¦¬ ê³„ì‚° (ë¯¸í„° ë‹¨ìœ„)
            max_distance = 0
            for lat, lng in current_coords:
                distance = geodesic((current_center_lat, current_center_lng), (lat, lng)).meters
                max_distance = max(max_distance, distance)
            
            # í´ëŸ¬ìŠ¤í„° ì› ê·¸ë¦¬ê¸° (ë°˜ê²½ì„ 20% ë” í¬ê²Œ)
            folium.Circle(
                location=(current_center_lat, current_center_lng),
                radius=max_distance * 1.2,  # 20% ë” í¬ê²Œ
                color='red',
                fill=True,
                fill_opacity=0.1,
                popup='í˜„ì¬ ì¶”ì²œ ì§€ì—­'
            ).add_to(current_group)
        
        # ë‹¤ìŒ ì¶”ì²œ ê·¸ë£¹ì˜ í´ëŸ¬ìŠ¤í„° ì› ê·¸ë¦¬ê¸°
        if next_coords:
            # ë‹¤ìŒ ì¶”ì²œ ê·¸ë£¹ì˜ ì¤‘ì‹¬ì  ê³„ì‚°
            next_center_lat = sum(lat for lat, _ in next_coords) / len(next_coords)
            next_center_lng = sum(lng for _, lng in next_coords) / len(next_coords)
            
            # ì¤‘ì‹¬ì ì—ì„œ ê°€ì¥ ë¨¼ ë§ˆì»¤ê¹Œì§€ì˜ ê±°ë¦¬ ê³„ì‚° (ë¯¸í„° ë‹¨ìœ„)
            max_distance = 0
            for lat, lng in next_coords:
                distance = geodesic((next_center_lat, next_center_lng), (lat, lng)).meters
                max_distance = max(max_distance, distance)
            
            # í´ëŸ¬ìŠ¤í„° ì› ê·¸ë¦¬ê¸° (ë°˜ê²½ì„ 20% ë” í¬ê²Œ)
            folium.Circle(
                location=(next_center_lat, next_center_lng),
                radius=max_distance * 1.2,  # 20% ë” í¬ê²Œ
                color='blue',
                fill=True,
                fill_opacity=0.1,
                popup='ë‹¤ìŒ ì‹œê°„ëŒ€ ì¶”ì²œ ì§€ì—­'
            ).add_to(next_group)
        
        # ë§ˆì»¤ ì¶”ê°€
        for data in valid_data:
            popup_content = f"""
                <div style='width: 200px'>
                    <b>{data['name']}</b><br>
                    ì¹´í…Œê³ ë¦¬: {data['category']}<br>
                    ì£¼ì†Œ: {data['address']}<br>
                    ìœ ì‚¬ë„: {data['similarity_score']}<br>
                    í˜„ì§€ì¸ì´ìš©ë¹„ì¤‘: {data['local_usage']}%
                </div>
            """
            
            if data['is_current']:
                folium.Marker(
                    [data['lat'], data['lng']],
                    popup=folium.Popup(popup_content, max_width=300),
                    icon=folium.Icon(color='red', icon='info-sign')
                ).add_to(current_group)
            else:
                folium.Marker(
                    [data['lat'], data['lng']],
                    popup=folium.Popup(popup_content, max_width=300),
                    icon=folium.Icon(color='blue', icon='info-sign')
                ).add_to(next_group)
        
        # ê·¸ë£¹ì„ ì§€ë„ì— ì¶”ê°€
        current_group.add_to(m)
        next_group.add_to(m)
        
        # ë ˆì´ì–´ ì»¨íŠ¸ë¡¤ ì¶”ê°€
        folium.LayerControl().add_to(m)
        
        logging.info(f"ì§€ë„ ìƒì„± ì™„ë£Œ - ì´ í‘œì‹œëœ ì‹ë‹¹ ìˆ˜: {len(valid_data)}")
        return m
        
    except Exception as e:
        logging.error(f"ì§€ë„ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        return None

def find_restaurants_by_keywords(df, user_keywords):
    """
    ì‚¬ìš©ì í‚¤ì›Œë“œì™€ ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œë¥¼ ê°€ì§„ ì‹ë‹¹ë“¤ì„ ì°¾ì•„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        matching_restaurants = []
        
        # ê° ì‹ë‹¹ì˜ í‚¤ì›Œë“œë¥¼ í™•ì¸
        for idx, row in df.iterrows():
            if isinstance(row.get('Keywords'), str):
                try:
                    restaurant_keywords = ast.literal_eval(row['Keywords'])
                except:
                    restaurant_keywords = {}
            else:
                restaurant_keywords = row.get('Keywords', {})
                
            # ì‚¬ìš©ì í‚¤ì›Œë“œê°€ ì‹ë‹¹ í‚¤ì›Œë“œì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if any(keyword in restaurant_keywords for keyword in user_keywords):
                matching_restaurants.append(idx)
                logging.info(f"ë§¤ì¹­ëœ ì‹ë‹¹: {row['ê°€ë§¹ì ëª…']}, í‚¤ì›Œë“œ: {restaurant_keywords}")
        
        if not matching_restaurants:
            logging.warning(f"í‚¤ì›Œë“œ {user_keywords}ì™€ ì¼ì¹˜í•˜ëŠ” ì‹ë‹¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
            
        return df.loc[matching_restaurants]
        
    except Exception as e:
        logging.error(f"í‚¤ì›Œë“œ ê¸°ë°˜ ì‹ë‹¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return pd.DataFrame()

def generate_response(prompt, result_df, gender_preference, time, category_type, keywords=None):
    try:
        # ì…ë ¥ê°’ ë¡œê¹…
        logging.info(f"\n{'='*50}\nì‘ë‹µ ìƒì„± ì‹œì‘\n{'='*50}")
        logging.info(f"ì…ë ¥ ì •ë³´:")
        logging.info(f"- ì›ë³¸ í”„ë¡¬í”„íŠ¸: {prompt}")
        logging.info(f"- ì„±ë³„ ì„ í˜¸: {gender_preference}")
        logging.info(f"- ì‹œê°„ëŒ€: {time}")
        logging.info(f"- ì¹´í…Œê³ ë¦¬: {category_type}")
        logging.info(f"- í‚¤ì›Œë“œ: {keywords}")
        
        if result_df is None or result_df.empty:
            logging.warning("ì¶”ì²œí•  ì‹ë‹¹ì´ ì—†ìŠµë‹ˆë‹¤.")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì¡°ê±´ì— ë§ëŠ” ì‹ë‹¹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        result_df = result_df.head(5)
        restaurant_info = ""
        
        for idx, row in result_df.iterrows():
            info = (f"### ê°€ê²Œ ì´ë¦„: {row['ê°€ë§¹ì ëª…']}\n"
                    f"- **ì£¼ì†Œ**: {row['ì£¼ì†Œ']}\n"
                    f"- **ì—…ì¢…**: {row['Categories']}\n"
                    f"- **ê°€ì„±ë¹„ ì—¬ë¶€**: {row['ê°€ì„±ë¹„ì—¬ë¶€']}\n"
                    f"- **ë³„ì **: {row['ë³„ì ']}\n"
                    f"- **ë°©ë¬¸ì ë¦¬ë·° ê°œìˆ˜**: {row['ë°©ë¬¸ìë¦¬ë·°ê°œìˆ˜']}\n"
                    f"- **ì˜ì—…ì‹œê°„**: {row['ì˜ì—…ì‹œê°„']}\n"
                    f"- **ì •ê¸°íœ´ì¼**: {row['ì •ê¸°íœ´ì¼']}\n"
                    f"- **íœ´ì¼ ê´€ë ¨ ì„¸ë¶€ì‚¬í•­**: {row['íœ´ì¼ê´€ë ¨ì„¸ë¶€ì‚¬í•­']}\n"
                    f"- **ë¸Œë ˆì´í¬íƒ€ì„**: {row['ë¸Œë ˆì´í¬íƒ€ì„']}\n"
                    f"- **ë¼ìŠ¤íŠ¸ì˜¤ë”íƒ€ì„**: {row['ë¼ìŠ¤íŠ¸ì˜¤ë”íƒ€ì„']}\n"
                    f"- **ìš”ì•½ëœ ë¦¬ë·°**: {row['ìš”ì•½ëœë¦¬ë·°']}\n"
                    f"- **Top-1 í‚¤ì›Œë“œ**: {row['Top-1_í‚¤ì›Œë“œ']}\n"
                    f"- **Top-2 í‚¤ì›Œë“œ**: {row['Top-2_í‚¤ì›Œë“œ']}\n"
                    f"- **Top-3 í‚¤ì›Œë“œ**: {row['Top-3_í‚¤ì›Œë“œ']}\n"
                    f"- **Top-5 ë©”ì¸ë©”ë‰´**: {row['Top-5_ë©”ì¸ë©”ë‰´']}\n")
            
            restaurant_info += info + "\n"

        full_prompt = f"""
        ë‹¹ì‹ ì€ ì œì£¼ë„ì˜ ë§›ì§‘ì„ ì˜ ì•„ëŠ” í˜„ì§€ ë§›ì§‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì‚¬ìš©ìì˜ ì„ í˜¸ë„ì™€ ì¡°ê±´ì„ ê³ ë ¤í•˜ì—¬ ë§›ì§‘ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

        ## ì…ë ¥ ì •ë³´
        - **ì‚¬ìš©ì ì§ˆë¬¸**: {prompt}
        - **ì„ í˜¸ ì‹œê°„ëŒ€**: {time}
        - **ì„ í˜¸ ì„±ë³„**: {gender_preference}
        - **ì„ í˜¸ ì—…ì¢…**: {category_type}
        {f'- **ê²€ìƒ‰ í‚¤ì›Œë“œ**: {", ".join(keywords)}' if keywords else ''}

        ## ì¶”ì²œ ì‹ë‹¹ ì •ë³´
        {restaurant_info}

        ## ë‹µë³€ í˜•ì‹
        ë‹¤ìŒ í˜•ì‹ì„ ë°˜ë“œì‹œ ì§€ì¼œì„œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

        1. ì²« ë¬¸ë‹¨: ì‚¬ìš©ìì˜ ìš”ì²­ì„ ìš”ì•½í•˜ê³  ì¶”ì²œí•˜ëŠ” ì‹ë‹¹ë“¤ì˜ ì „ë°˜ì ì¸ íŠ¹ì§• ì†Œê°œ
           - ì¡´ëŒ“ë§ ì‚¬ìš©
           - ê° ì‹ë‹¹ ì´ë¦„ì€ **ë³¼ë“œì²´**ë¡œ ê°•ì¡°
           - ê°„ë‹¨í•œ íŠ¹ì§• ì–¸ê¸‰

        2. êµ¬ë¶„ì„  ì¶”ê°€: "---"

        3. ê° ì‹ë‹¹ë³„ ìƒì„¸ ì„¤ëª… (ëª¨ë“  ì¶”ì²œ ì‹ë‹¹ì— ëŒ€í•´ ë°˜ë³µ):
           ### ğŸ† **[ì‹ë‹¹ì´ë¦„]**
           
           - ğŸ“ **ìœ„ì¹˜**: [ì£¼ì†Œ]
           - ğŸ•’ **ì˜ì—… ì‹œê°„**: [ì˜ì—…ì‹œê°„]
           - â° **ë¸Œë ˆì´í¬íƒ€ì„**: [ë¸Œë ˆì´í¬íƒ€ì„]
           - ğŸ”š **ë¼ìŠ¤íŠ¸ì˜¤ë”**: [ë¼ìŠ¤íŠ¸ì˜¤ë”íƒ€ì„]
           - ğŸ“… **ì •ê¸°íœ´ì¼**: [ì •ê¸°íœ´ì¼]
           - â­ **ë³„ì **: [ë³„ì ]
           - ğŸ‘¥ **ë°©ë¬¸ì ë¦¬ë·°**: [ë°©ë¬¸ìë¦¬ë·°ê°œìˆ˜]ê±´
           
           **ëŒ€í‘œ ë©”ë‰´**:
           [Top-5_ë©”ì¸ë©”ë‰´]
           
           **ì£¼ìš” í‚¤ì›Œë“œ**:
           - [Top-1_í‚¤ì›Œë“œ]
           - [Top-2_í‚¤ì›Œë“œ]
           - [Top-3_í‚¤ì›Œë“œ]
           
           **ë¦¬ë·° ìš”ì•½**:
           [ìš”ì•½ëœë¦¬ë·°]
           
           **ì¶”ì²œ ì´ìœ **:
           1. **[íŠ¹ì§•1]**
              ìƒì„¸ ì„¤ëª…...
           2. **[íŠ¹ì§•2]**
              ìƒì„¸ ì„¤ëª…...

           ---

        4. ë§ˆë¬´ë¦¬ ë©˜íŠ¸:
           - ëª¨ë“  ì¶”ì²œ ì‹ë‹¹ë“¤ì˜ ê³µí†µì ì¸ ì¥ì  ì–¸ê¸‰
           - ë°©ë¬¸ ì‹œ ê¸°ëŒ€í•  ìˆ˜ ìˆëŠ” ì 
           - ê¸ì •ì  ë§ˆë¬´ë¦¬

        ì£¼ì˜ì‚¬í•­:
        - ê° ì‹ë‹¹ì˜ íŠ¹ì§•ì€ **ë³¼ë“œì²´**ë¡œ ê°•ì¡°
        - êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ì •ë³´ í¬í•¨
        - ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤ ìœ ì§€
        - ì£¼ì–´ì§„ ì •ë³´ë¥¼ ê·¸ëŒ€ë¡œ í™œìš©í•˜ì—¬ ì‘ì„±(ì •ë³´ê°€ ì—†ë‹¤ë©´ í•´ë‹¹ ë‚´ìš© ì¶œë ¥í•˜ì§€ ë§ ê²ƒ)
        - ì‹¤ì œ ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ê°€í•˜ì§€ ë§ ê²ƒ
        - ëª¨ë“  ì¶”ì²œ ì‹ë‹¹ì— ëŒ€í•´ ê· í˜•ìˆê²Œ ì„¤ëª…í•  ê²ƒ
        """
        
        # í† í° ìˆ˜ ê³„ì‚° ë° ë¡œê¹… ì¶”ê°€
        total_tokens = model.count_tokens(full_prompt)
        logging.info(f"ì…ë ¥ í† í° ìˆ˜: {total_tokens}")
        
        logging.info(f"Gemini í”„ë¡¬í”„íŠ¸ ì…ë ¥: {full_prompt}")
        
        # LLM ìš”ì²­ ì „ ì‹œê°„ ê¸°ë¡
        start_time = time_module.time()
        
        # í–¥ìƒëœ LLM ìš”ì²­ ì„¤ì •
        response = model.generate_content(
            full_prompt
        )
        
        # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        end_time = time_module.time()
        response_time = end_time - start_time
        
        if not response or not response.text:
            logging.error("LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        # ì‘ë‹µ ë¡œê¹…
        logging.info(f"\nLLM ì‘ë‹µ ì •ë³´:")
        logging.info(f"- ì‘ë‹µ ì‹œê°„: {response_time:.2f}ì´ˆ")
        logging.info(f"- ì‘ë‹µ ë‚´ìš©: {response.text[:200]}...")  # ì²˜ìŒ 200ìë§Œ ë¡œê¹…

        return response.text
        
    except Exception as e:
        logging.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

def filter_by_keywords(df, keywords, threshold=0.3):
    """
    í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°í”„ë ˆì„ì„ í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        logging.info(f"í‚¤ì›Œë“œ í•„í„°ë§ ì‹œì‘ - ì›ë³¸ í‚¤ì›Œë“œ: {keywords}")
        if not keywords or df.empty:
            return df

        # í‚¤ì›Œë“œ ì„ë² ë”©ì„ ê³„ì‚°
        keyword_embeddings = [embed_text(keyword) for keyword in keywords]
        keyword_embeddings = np.array(keyword_embeddings, dtype=np.float32)
        faiss.normalize_L2(keyword_embeddings)

        filtered_rows = []
        similarities = []

        for idx, row in df.iterrows():
            try:
                # ê° í–‰ì˜ í‚¤ì›Œë“œ ì„ë² ë”©ì„ ê°€ì ¸ì˜´
                top1_emb = str_to_array(row['Top-1_í‚¤ì›Œë“œ_ì„ë² ë”©'])
                top2_emb = str_to_array(row['Top-2_í‚¤ì›Œë“œ_ì„ë² ë”©'])
                top3_emb = str_to_array(row['Top-3_í‚¤ì›Œë“œ_ì„ë² ë”©'])

                # nan ê°’ ì²˜ë¦¬
                if np.isnan(top1_emb).any() or np.isnan(top2_emb).any() or np.isnan(top3_emb).any():
                    logging.error(f"ì„ë² ë”© ë³€í™˜ ì˜¤ë¥˜: {row['ê°€ë§¹ì ëª…']}ì˜ ì„ë² ë”©ì— nan ê°’ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                    continue

                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                row_embeddings = np.array([top1_emb, top2_emb, top3_emb], dtype=np.float32)
                faiss.normalize_L2(row_embeddings)

                # FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ê²€ìƒ‰
                index = faiss.IndexFlatIP(row_embeddings.shape[1])
                index.add(row_embeddings)

                # ìœ ì‚¬ë„ ê²€ìƒ‰
                sim, _ = index.search(keyword_embeddings, 1)
                max_similarity = np.max(sim)

                logging.info(f"í–‰ {idx} - ìµœëŒ€ ìœ ì‚¬ë„: {max_similarity:.4f}")

                if max_similarity > threshold:
                    filtered_rows.append(idx)
                    similarities.append(max_similarity)

            except Exception as e:
                logging.error(f"í–‰ {idx} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                continue

        if not filtered_rows:
            logging.warning("í‚¤ì›Œë“œ ë§¤ì¹­ ê²°ê³¼ ì—†ìŒ")
            return df

        filtered_df = df.loc[filtered_rows].copy()
        filtered_df['keyword_similarity'] = similarities
        filtered_df = filtered_df.sort_values('keyword_similarity', ascending=False)

        logging.info(f"í‚¤ì›Œë“œ í•„í„°ë§ ê²°ê³¼: {len(filtered_df)}ê°œ í•­ëª© ë§¤ì¹­")
        return filtered_df

    except Exception as e:
        logging.error(f"í‚¤ì›Œë“œ í•„í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        return df

def filter_data_by_criteria(df, location, category, time_periods=None, conditions=None, gender_preference=None):
    try:
        logging.info(f"\n{'='*50}\ní•„í„°ë§ í”„ë¡œì„¸ìŠ¤ ì‹œì‘\n{'='*50}")
        logging.info(f"ì…ë ¥ íŒŒë¼ë¯¸í„°:\n- ìœ„ì¹˜: {location}\n- ì¹´í…Œê³ ë¦¬: {category}\n- ì‹œê°„ëŒ€: {time_periods}\n- ì¡°ê±´: {conditions}\n- ì„±ë³„ ì„ í˜¸: {gender_preference}")
        logging.info(f"ì´ˆê¸° ë°ì´í„° í¬ê¸°: {len(df)} í–‰")
        
        filtered_df = df.copy()
        
        # 1. FAISS ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰
        logging.info("1. FAISS ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ ì‹œì‘")
        
        # ë¬¸ìì—´ì„ numpy ì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
        def parse_embedding(embedding_str):
            # ë¬¸ìì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
            numbers = [float(num) for num in embedding_str.strip('[]').split()]
            return np.array(numbers)

        try:
            # CSV íŒŒì¼ ì½ê¸° ì‹œ ì¸ì½”ë”© ì„¤ì •
            if 'ì¥ì†Œ_ì„ë² ë”©' not in filtered_df.columns:
                embedding_column = [col for col in filtered_df.columns if 'ì„ë² ë”©' in col][0]
                filtered_df = filtered_df.rename(columns={embedding_column: 'ì¥ì†Œ_ì„ë² ë”©'})
            
            # ë°ì´í„°í”„ë ˆì„ì˜ ì„ë² ë”©ì„ numpy ì—´ë¡œ ë³€í™˜
            embeddings = []
            for emb_str in filtered_df['ì¥ì†Œ_ì„ë² ë”©'].values:
                try:
                    numbers = [float(num) for num in emb_str.strip('[]').split()]
                    embeddings.append(numbers)
                except Exception as e:
                    logging.error(f"ê°œë³„ ì„ë² ë”© ë³€í™˜ ì˜¤ë¥˜: {e}")
                    embeddings.append(np.zeros(768))
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ê³  float32 íƒ€ì…ìœ¼ë¡œ ëª…ì‹œì  ë³€í™˜
            embeddings = np.array(embeddings, dtype=np.float32)
            logging.info(f"ì„ë² ë”© ë°°ì—´ shape: {embeddings.shape}")

            # FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ë²¡í„° ì¶”ê°€
            dimension = embeddings.shape[1]
            index = faiss.IndexFlatIP(dimension)
            
            # L2 ì •ê·œí™” ì „ì— ë³µì‚¬ë³¸ ìƒì„±
            embeddings_normalized = embeddings.copy()
            faiss.normalize_L2(embeddings_normalized)
            
            # ì •ê·œí™”ëœ ë²¡í„° ì¶”ê°€
            index.add(embeddings_normalized)

            # ì¿¼ë¦¬ ì„ë² ë”© ì¤€ë¹„
            location_mask = filtered_df['íŒŒì‹±ëœ_ì¥ì†Œ'].str.contains(location, na=False)
            if location_mask.any():
                location_emb = parse_embedding(filtered_df[location_mask].iloc[0]['ì¥ì†Œ_ì„ë² ë”©'])
                query_emb = location_emb.reshape(1, -1).astype(np.float32)
                
                # ì¿¼ë¦¬ ë²¡í„° ì •ê·œí™”
                query_normalized = query_emb.copy()
                faiss.normalize_L2(query_normalized)

                # ìœ ì‚¬ë„ ê²€ìƒ‰
                k = min(1000, len(filtered_df))
                similarities, indices = index.search(query_normalized, k)

                logging.info(f"FAISS ê²€ìƒ‰ ê²°ê³¼ - ìµœëŒ€ ìœ ì‚¬ë„: {similarities[0][0]:.4f}, ìµœì†Œ ìœ ì‚¬ë„: {similarities[0][-1]:.4f}")

                # ìœ ì‚¬ë„ ì„ê³„ê°’ ì ìš©
                threshold = 0.3
                valid_indices = indices[0][similarities[0] > threshold]
                filtered_df = filtered_df.iloc[valid_indices].copy()
                filtered_df['similarity_score'] = similarities[0][similarities[0] > threshold]

                logging.info(f"ì„ë² ë”© ê¸°ë°˜ í•„í„°ë§ í›„ ë°ì´í„° ìˆ˜: {len(filtered_df)}")
            else:
                logging.warning(f"ìœ„ì¹˜ '{location}'ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()

        except Exception as e:
            logging.error(f"ì„ë² ë”© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return pd.DataFrame()

        # 2. ì‹œê°„ëŒ€ í•„í„°ë§
        if time_periods:
            logging.info("2. ì‹œê°„ëŒ€ í•„í„° ì‹œì‘")
            time_mapping = {
                'ì•„ì¹¨': '5ì‹œ11ì‹œì´ìš©ê±´ìˆ˜ë¹„ì¤‘',
                'ì ì‹¬': '12ì‹œ13ì‹œì´ìš©ê±´ìˆ˜ë¹„ì¤‘',
                'ì˜¤í›„': '14ì‹œ17ì‹œì´ìš©ê±´ìˆ˜ë¹„ì¤‘',
                'ì €ë…': '18ì‹œ22ì‹œì´ìš©ê±´ìˆ˜ë¹„ì¤‘',
                'ë°¤': '23ì‹œ4ì‹œì´ìš©ê±´ìˆ˜ë¹„ì¤‘' 
            }
            
            filtered_df['ì‹œê°„ëŒ€_ì ìˆ˜'] = 0
            valid_times = []
            
            for t in time_periods:
                if t in time_mapping:
                    column = time_mapping[t]
                    if column in filtered_df.columns:
                        # ë°ì´í„° íƒ€ì…ì´ ìˆ«ìí˜•ì¸ì§€ í™•ì¸í•˜ê³  ë³€í™˜
                        filtered_df[column] = filtered_df[column].astype(float)
                        filtered_df['ì‹œê°„ëŒ€_ì ìˆ˜'] += filtered_df[column]
                        valid_times.append(t)
                    else:
                        logging.warning(f"ì»¬ëŸ¼ '{column}'ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŠµë‹ˆë‹¤.")
            
            if valid_times:
                temp_df = filtered_df[filtered_df['ì‹œê°„ëŒ€_ì ìˆ˜'] > 0]
                if not temp_df.empty:
                    filtered_df = temp_df
                    filtered_df = filtered_df.sort_values('ì‹œê°„ëŒ€_ì ìˆ˜', ascending=False)
                    logging.info(f"ì‹œê°„ëŒ€ í•„í„°ë§ í›„ ë°ì´í„° ìˆ˜: {len(filtered_df)}")
                else:
                    logging.warning("ì‹œê°„ëŒ€ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë°ì´í„°ê°€ ì—†ì–´ ì „ì²´ ë°ì´í„°ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.")
            else:
                logging.warning("ìœ íš¨í•œ ì‹œê°„ëŒ€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì„±ë³„ í•„í„°ë§ ì¶”ê°€
        if gender_preference:
            before_count = len(filtered_df)
            if isinstance(gender_preference, list):
                gender_preference = gender_preference[0]
            
            # 'ë‚¨ì„± ì´ìš© ë¹„ì¤‘ ë†’ìŒ' ë˜ëŠ” 'ì—¬ì„± ì´ìš© ë¹„ì¤‘ ë†’ìŒ' ì¡°ê±´ ì¶”ê°€
            if gender_preference.lower() in ["ë‚¨ì„±", "ë‚¨ì„± ì´ìš© ë¹„ì¤‘ ë†’ìŒ"]:
                filtered_df = filtered_df[filtered_df['ìµœê·¼12ê°œì›”ë‚¨ì„±íšŒì›ìˆ˜ë¹„ì¤‘'] > filtered_df['ìµœê·¼12ê°œì›”ì—¬ì„±íšŒì›ìˆ˜ë¹„ì¤‘']]
            elif gender_preference.lower() in ["ì—¬ì„±", "ì—¬ì„± ì´ìš© ë¹„ì¤‘ ë†’ìŒ"]:
                filtered_df = filtered_df[filtered_df['ìµœê·¼12ê°œì›”ì—¬ì„±íšŒì›ìˆ˜ë¹„ì¤‘'] > filtered_df['ìµœê·¼12ê°œì›”ë‚¨ì„±íšŒì›ìˆ˜ë¹„ì¤‘']]
            
            after_count = len(filtered_df)
            logging.info(f"ì„±ë³„ '{gender_preference}' í•„í„°ë§ ê²°ê³¼: {before_count} -> {after_count} í–‰")
        
        # 3. ì¡°ê±´ í•„í„°ë§
        if conditions:
            logging.info("3. ì¶”ê°€ ì¡°ê±´ í•„í„°ë§ ì‹œì‘")
            for condition in conditions:
                temp_df = filtered_df.copy()
                condition = condition.lower()
                
                # ê°€ì„±ë¹„ ì—¬ë¶€ ì¡°ê±´ ì¶”ê°€
                if "ê°€ì„±ë¹„" in condition:
                    temp = temp_df[temp_df['ê°€ì„±ë¹„ì—¬ë¶€'] == 1]
                    if not temp.empty:
                        filtered_df = temp

                # ì´ìš© ê±´ìˆ˜ ì¡°ê±´
                if "ìƒìœ„ 10%" in condition:
                    temp = temp_df[temp_df['ì´ìš©ê±´ìˆ˜êµ¬ê°„'].isin(['5_75~90%', '6_90~100%'])]
                    if not temp.empty:
                        filtered_df = temp
                
                # í˜„ì§€ì¸ ì¡°ê±´
                elif "í˜„ì§€ì¸" in condition:
                    temp_df = temp_df.sort_values('í˜„ì§€ì¸ì´ìš©ê±´ìˆ˜ë¹„ì¤‘', ascending=False)
                    temp = temp_df.head(max(int(len(temp_df) * 0.2), 1))
                    if not temp.empty:
                        filtered_df = temp
                
                # ì—°ë ¹ëŒ€ ì¡°ê±´
                elif any(age in condition for age in ['20ëŒ€', '30ëŒ€', '40ëŒ€', '50ëŒ€', '60ëŒ€']):
                    age_mapping = {
                        '20ëŒ€': 'ìµœê·¼12ê°œì›”20ëŒ€ì´í•˜íšŒì›ìˆ˜ë¹„ì¤‘',
                        '30ëŒ€': 'ìµœê·¼12ê°œì›”30ëŒ€íšŒì›ìˆ˜ë¹„ì¤‘',
                        '40ëŒ€': 'ìµœê·¼12ê°œì›”40ëŒ€íšŒì›ìˆ˜ë¹„ì¤‘',
                        '50ëŒ€': 'ìµœê·¼12ê°œì›”50ëŒ€íšŒì›ìˆ˜ë¹„ì¤‘',
                        '60ëŒ€': 'ìµœê·¼12ê°œì›”60ëŒ€ì´ìƒíšŒì›ìˆ˜ë¹„ì¤‘'
                    }
                    
                    for age, column in age_mapping.items():
                        if age in condition:
                            # ì¡°ê±´ì—ì„œ í¼ì„¼íŠ¸ ê°’ì„ ì¶”ì¶œ
                            percent_match = re.search(r'(\d+)%', condition)
                            threshold = float(percent_match.group(1)) if percent_match else 20.0
                            
                            temp = temp_df[temp_df[column] >= threshold]
                            if not temp.empty:
                                filtered_df = temp
                                logging.info(f"{age} ì—°ë ¹ëŒ€ {threshold}% ì´ìƒ í•„í„°ë§ ê²°ê³¼: {len(temp)}ê°œ ì‹ë‹¹")
                            else:
                                logging.warning(f"{age} ì—°ë ¹ëŒ€ {threshold}% ì´ìƒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì‹ë‹¹ì´ ì—†ìŠµë‹ˆë‹¤")
                            break
                
                logging.info(f"ì¡°ê±´ '{condition}' ì ìš© í›„ ë°ì´í„° ìˆ˜: {len(filtered_df)}")

        # ê° í•„í„°ë§ ë‹¨ê³„ë§ˆë‹¤ ìƒì„¸ ë¡œê¹… ì¶”ê°€
        if time_periods:
            for time in time_periods:
                before_count = len(filtered_df)
                # ... ê¸°ì¡´ ì‹œê°„ëŒ€ í•„í„°ë§  ...
                after_count = len(filtered_df)
                logging.info(f"ì‹œê°„ëŒ€ '{time}' í•„í„°ë§ ê²°ê³¼: {before_count} -> {after_count} í–‰")
        
        # ì¡°ê±´ í•„í„°ë§ì— ìƒì„¸ ë¡œê¹… ì¶”ê°€
        if conditions:
            for condition in conditions:
                before_count = len(filtered_df)
                # ... ê¸°ì¡´ ì¡°ê±´ í•„í„°ë§ ë¡œì§ ...
                after_count = len(filtered_df)
                logging.info(f"ì¡°ê±´ '{condition}' í•„í„°ë§ ê²°ê³¼: {before_count} -> {after_count} í–‰")
                
                # ì¡°ê±´ë³„ ìƒì„¸ ì •ë³´ ë¡œê¹…
                if 'ì´ìš©ê±´ìˆ˜ ìƒìœ„' in condition.lower():
                    logging.info(f"ì´ìš©ê±´ìˆ˜ êµ¬ê°„ ë¶„í¬:\n{filtered_df['ì´ìš©ê±´ìˆ˜êµ¬ê°„'].value_counts()}")
                elif 'í˜„ì§€ì¸' in condition.lower():
                    logging.info(f"í˜„ì§€ ì´ìš© ë¹„ì¤‘ í†µê³„:\n- í‰ê· : {filtered_df['í˜„ì§€ì¸ì´ìš©ê±´ìˆ˜ë¹„ì¤‘'].mean():.2f}\n- ìµœëŒ€: {filtered_df['í˜„ì§€ì¸ì´ìš©ê±´ìˆ˜ë¹„ì¤‘'].max():.2f}")
        
        # ìµœì¢… ê²°ê³¼ ëŒ€í•œ ìƒì„¸ ì •ë³´ ë¡œê¹…
        logging.info(f"\n{'='*50}\nìµœì¢… ê²°ê³¼ ìƒì„¸ ì •ë³´\n{'='*50}")
        for idx, row in filtered_df.head().iterrows():
            logging.info(f"\nì‹ë‹¹ {idx+1} ìƒì„¸ì •ë³´:")
            logging.info(f"- ì´ë¦„: {row['ê°€ë§¹ì ëª…']}")
            logging.info(f"- ì£¼ì†Œ: {row['ì£¼ì†Œ']}")
            logging.info(f"- ì¹´í…Œê³ ë¦¬: {row['Categories']}")
            logging.info(f"- ìœ ì‚¬ë„ ì ìˆ˜: {row.get('similarity_score', 'N/A'):.4f}")
            logging.info(f"- í˜„ì§€ì¸ ì´ìš© ë¹„ì¤‘: {row['í˜„ì§€ì¸ì´ìš©ê±´ìˆ˜ë¹„ì¤‘']:.2f}%")
            if time_periods:
                for time in time_periods:
                    if time == 'ì•„ì¹¨':
                        logging.info(f"- ì•„ì¹¨ ì´ìš© ë¹„ì¤‘: {row['5ì‹œ11ì‹œì´ìš©ê±´ìˆ˜ë¹„ì¤‘']:.2f}%")
                    elif time == 'ì ì‹¬':
                        logging.info(f"- ì ì‹¬ ì´ìš© ë¹„ì¤‘: {row['12ì‹œ13ì‹œì´ìš©ê±´ìˆ˜ë¹„ì¤‘']:.2f}%")
                    elif time == 'ì˜¤í›„':
                        logging.info(f"- ì˜¤í›„ ì´ìš© ë¹„ì¤‘: {row['14ì‹œ17ì‹œì´ìš©ê±´ìˆ˜ë¹„ì¤‘']:.2f}%")
                    elif time == 'ì €ë…':
                        logging.info(f"- ì €ë… ì´ìš© ë¹„ì¤‘: {row['18ì‹œ22ì‹œì´ìš©ê±´ìˆ˜ë¹„ì¤‘']:.2f}%")
                    elif time == 'ë°¤':
                        logging.info(f"- ì‹¬ì•¼ ì´ìš© ë¹„ì¤‘: {row['23ì‹œ4ì‹œì´ìš©ê±´ìˆ˜ë¹„ì¤‘']:.2f}%")
        
        return filtered_df.head(5)  # ìµœì¢… ê²°ê³¼ ë°˜í™˜ (ìµœëŒ€ 5ê°œ)

    except Exception as e:
        logging.error(f"í•„í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        return filtered_df.head(5)  # ì˜¤ë¥˜ ë°œìƒì‹œ í˜„ì¬ê¹Œì§€ì˜ ê²°ê³¼ ë°˜í™˜

# í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± í•¨ìˆ˜
def get_embedding(text):
    try:
        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
        if torch.cuda.is_available():
            inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = embedding_model(**inputs)
        
        # [CLS] í† í°ì˜ ì„ë² ë”© ì‚¬ìš©
        embedding = outputs.last_hidden_state[0][0].cpu().numpy()
        return embedding
        
    except Exception as e:
        logging.error(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return np.zeros(768)  # ê¸°ë³¸ ì„ë² ë”© ì°¨ì›

def parse_user_input_to_json(user_input, retries=3):
    
    prompt = f"""
    ì œì£¼ë„ ë§›ì§‘ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ì…ë ¥ì„ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.
    ë‹¤ìŒ ë‹¨ê³„ì— ë”°ë¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
    1. ì¥ì†Œ ì •ë³´ ì¶”ì¶œ
    - ì œì£¼ì‹œ/ì„œê·€í¬ì‹œ êµ¬ë¶„
    - ì/ë©´/ë™ ë‹¨ìœ„ê¹Œì§€ í¬í•¨
    - ì˜ˆ: "ì œì£¼ì‹œ ë…¸í˜•ë™", "ì„œê·€í¬ì‹œ ìƒ‰ë‹¬ë™"
    2. ì—…ì¢… ë¶„ë¥˜
    - ì£¼ìš” ì¹´í…Œê³ ë¦¬: "ìœ¡ë¥˜,ê³ ê¸°ìš”ë¦¬", "í•´ë¬¼,ìƒì„ ìš”ë¦¬", "ê°€ì •ì‹", "ë‹¨í’ˆìš”ë¦¬ " ë“±
    - ì²´ ì—…ì¢…ìœ¼ë¡œ ë§¤í•‘
    3. ì‹œê°„ëŒ€ ë¶„ì„
    - ì•„ì¹¨(5-11ì‹œ), ì ì‹¬(12-13ì‹œ), ì €ë…(17-21ì‹œ), ë°¤(22-23ì‹œ)
    - í•´ë‹¹í•˜ëŠ” ì‹œê°„ëŒ€ ë°°ì—´ë¡œ í‘œí˜„
    4. ì„±ë³„ ë¶„ì„
    - ë‚¨ì„± ì´ìš© ë¹„ì¤‘ì´ ë†’ì€ ê²½ìš°: "ë‚¨ì„± ì´ìš© ë¹„ì¤‘ ë†’ìŒ"
    - ì—¬ì„± ì´ìš© ë¹„ì¤‘ì´ ë†’ì€ ê²½ìš°: "ì—¬ì„± ì´ìš© ë¹„ì¤‘ ë†’ìŒ"
    - ì„±ë³„ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ í‘œí˜„
    5. ì¡°ê±´ ë¶„ì„ (ì•„ë˜ ì¡°ê±´ë§Œ ì‚¬ìš© ê°€ëŠ¥)
    - ì´ìš©ê±´ìˆ˜: "ì´ìš©ê±´ìˆ˜ ìƒìœ„ 10%", "ì´ìš©ê±´ìˆ˜ ìƒìœ„ 20%"
    - ì—°ë ¹ëŒ€: 
        "20ëŒ€ ì´í•˜ ì´ìš© ë¹„ì¤‘ 10% ì´ìƒ", "30ëŒ€ ì´ìš© ë¹„ì¤‘ 30% ì´ìƒ", "50ëŒ€ ì´ìš© ë¹„ì¤‘ 20% ì´ìƒ" ë“±
    - ìš”ì¼: "ì›”ìš”ì¼ ì´ìš© ë¹„ì¤‘ ê°€ì¥ ë†’ìŒ", "í™”ìš”ì¼ ì´ìš© ë¹„ì¤‘ ê°€ì¥ ë†’ìŒ" ë“±
    - í˜„ì§€ì¸: "í˜„ì§€ì¸ ì´ìš© ë¹„ì¤‘ ê°€ì¥ ë†’ìŒ"
    - ê°€ì„±ë¹„: "ê°€ì„±ë¹„"
    6. í‚¤ì›Œë“œ ì¡°ê±´
    - ìŒì‹ì ì˜ íŠ¹ì§•ì„ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ
    - ì˜ˆ: "ìŒì‹ì´ ë§›ìˆì–´ìš”", "ê°€ì„±ë¹„ê°€ ì¢‹ì•„ìš”", "ì–‘ì´ ë§ì•„ìš”", "ì¹œì ˆí•´ìš”" ë“±
    ì¶œë ¥ í˜•ì‹:
    {{
        "ì¥ì†Œ": "ì§€ì—­ëª…",
        "Categories": "ì—…ì¢…",
        "ì‹œê°„ëŒ€": ["ì•„ì¹¨", "ì ì‹¬", "ì €ë…", "ë°¤"],
        "ì„±ë³„": ["ì„±ë³„ ì¡°ê±´"],
        "ì¡°ê±´": ["ì¡°ê±´1", "ì¡°ê±´2", ...],
        "í‚¤ì›Œë“œ": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...]
    }}
    ì˜ˆì‹œ ì…ë ¥/ì¶œë ¥:
    1. ì…ë ¥: "ì œì£¼ì‹œ ë…¸í˜•ë™ì—ì„œ ì´ìš©ê°ì´ ë§ê³  ë§›ìˆëŠ” ê°€ì„±ë¹„ ê³ ê¸°ì§‘ ì¶”ì²œí•´ì¤˜"
    â†’ {{
        "ì¥ì†Œ": "ì œì£¼ì‹œ ë…¸í˜•ë™",
        "Categories": "ìœ¡ë¥˜,ê³ ê¸°ìš”ë¦¬",
        "ì‹œê°„ëŒ€": [],
        "ì„±ë³„": [],
        "ì¡°ê±´": ["ì´ìš©ê±´ìˆ˜ ìƒìœ„ 10%", "ê°€ì„±ë¹„"],
        "í‚¤ì›Œë“œ": ["ìŒì‹ì´ ë§›ìˆì–´ìš”", "ê³ ê¸° ì§ˆì´ ì¢‹ì•„ìš”"]
    }}
    2. ì…ë ¥: "ì„œê·€í¬ì‹œ ì¤‘ë¬¸ë™ì—ì„œ í˜„ì§€ì¸ë“¤ì´ ìì£¼ê°€ê³  ì£¼ë§ì— ì¸ê¸°ìˆëŠ” í•´ì‚°ë¬¼ì§‘ ì•Œë ¤ì¤˜"
    â†’ {{
        "ì¥ì†Œ": "ì„œê·€í¬ì‹œ ì¤‘ë¬¸ë™",
        "Categories": "í•´ë¬¼,ìƒì„ ìš”ë¦¬",
        "ì‹œê°„ëŒ€": [],
        "ì„±ë³„": [],
        "ì¡°ê±´": ["í˜„ì§€ì¸ ì´ìš© ë¹„ì¤‘ ê°€ì¥ ë†’ìŒ", "í† ìš”ì¼ ì´ìš© ë¹„ì¤‘ ê°€ì¥ ë†’ìŒ"],
        "í‚¤ì›Œë“œ": ["ìŒì‹ì´ ë§›ìˆì–´ìš”", "ì¬ë£Œê°€ ì‹ ì„ í•´ìš”"]
    }}
    3. ì…ë ¥: "ë‚˜ëŠ” ê°€ì¥ì¸ë° ì œì£¼ì‹œ ì—°ë™ì—ì„œ ì €ë…ì— ê°€ì¡±ë“¤ê³¼ ê¹”ë”í•œ í•œì‹ì§‘ ì¶”ì²œí•´ì¤˜"
    â†’ {{
        "ì¥ì†Œ": "ì œì£¼ì‹œ ì—°ë™",
        "Categories": "ê°€ì •ì‹",
        "ì‹œê°„ëŒ€": ["ì €ë…"],
        "ì„±ë³„": ["ë‚¨ì„± ì´ìš© ë¹„ì¤‘ ë†’ìŒ"],
        "ì¡°ê±´": ["50ëŒ€ ì´ìš© ë¹„ì¤‘ 10% ì´ìƒ", "ì´ìš©ê±´ìˆ˜ ìƒìœ„ 20%"],
        "í‚¤ì›Œë“œ": ["ë§¤ì¥ì´ ì²­ê²°í•´ìš”", "ê°€ì¡±ëª¨ì„ í•˜ê¸° ì¢‹ì•„ìš”"]
    }}
    4. ì…ë ¥: "ì ì‹¬ì— ì—¬ìì¸ ì¹œêµ¬ë“¤ì´ë‘ ì„œê·€í¬ì‹œ ìƒ‰ë‹¬ë™ì—ì„œ 20ëŒ€ê°€ ë§ì´ ê°€ê³  ê¸ˆìš”ì¼ì— ì¸ê¸°ìˆëŠ” ë¶„ìœ„ê¸° ì¢‹ì€ ë§›ì§‘ ì•Œë ¤ì¤˜"
    â†’ {{
        "ì¥ì†Œ": "ì„œê·€ì‹œ ìƒ‰ë‹¬ë™",
        "Categories": "ë‹¨í’ˆìš”ë¦¬ ì „ë¬¸",
        "ì‹œê°„ëŒ€": ["ì ì‹¬"],
        "ì„±ë³„": ["ì—¬ì„± ì´ìš© ë¹„ì¤‘ ë†’ìŒ"],
        "ì¡°ê±´": ["20ëŒ€ ì´ìš© ë¹„ì¤‘ 30% ì´ìƒ", "ê¸ˆìš”ì¼ ì´ìš© ë¹„ì¤‘ ê°€ì¥ ë†’ìŒ", "ê°€ì„±ë¹„"],
        "í‚¤ì›Œë“œ": ["ì¸í…Œë¦¬ì–´ê°€ ë©‹ì ¸ìš”", "ë¶„ìœ„ê¸°ê°€ ì¢‹ì•„ìš”"]
    }}
    5. ì…ë ¥: "ì œì£¼ì‹œ ì´ë„ë™ì—ì„œ ì ì‹¬ì— ë‚¨ì„± ì§ì¥ì¸ì´ ë§ì´ ê°€ëŠ” ë§›ì§‘ ì¶”ì²œí•´ì¤˜"
    â†’ {{
        "ì¥ì†Œ": "ì œì£¼ì‹œ ì´ë„ë™",
        "Categories": "ë‹¨í’ˆìš”ë¦¬ ì „ë¬¸",
        "ì‹œê°„ëŒ€": ["ì ì‹¬"],
        "ì„±ë³„": ["ë‚¨ì„± ì´ìš© ë¹„ì¤‘ ë†’ìŒ"],
        "ì¡°ê±´": ["30ëŒ€ ì´ìš© ë¹„ì¤‘ 20% ì´ìƒ", "ì´ìš©ê±´ìˆ˜ ìƒìœ„ 10%"],
        "í‚¤ì›Œë“œ": ["í˜¼ë°¥í•˜ê¸° ì¢‹ì•„ìš”", "ìŒì‹ì´ ë§›ìˆì–´ìš”"]
    }}
    6. ì…ë ¥: "ì• ì›”ìì—ì„œ í˜„ì§€ì¸ì´ ì¶”ì²œí•˜ëŠ” ì¸ê¸°ë§ì€ ì˜¤ì…˜ë·° ë§›ì§‘ ì•Œë ¤ì¤˜"
    â†’ {{
        "ì¥ì†Œ": "ì œì£¼ì‹œ ì• ì›”ì",
        "Categories": "ë‹¨í’ˆìš”ë¦¬ ì „ë¬¸",
        "ì‹œê°„ëŒ€": [],
        "ì„±ë³„": [],
        "ì¡°ê±´": ["í˜„ì§€ì¸ ì´ìš© ë¹„ì¤‘ ê°€ì¥ ë†’ìŒ", "ì´ìš©ê±´ìˆ˜ ìƒìœ„ 10%"],
        "í‚¤ì›Œë“œ": ["ë·°ê°€ ì¢‹ì•„ìš”", "ìŒì‹ì´ ë§›ìˆì–´ìš”"]
    }}
    7. ì…ë ¥: "ì œì£¼ì‹œ êµ¬ì¢Œìì—ì„œ ì•„ì¹¨ì¼ì° ë¬¸ì—¬ëŠ” ì‹ ì„ í•œ í•´ì‚°ë¬¼ì§‘ ì¤‘ì— ì¸ê¸°ìˆëŠ” ê³³ ì¶”ì²œí•´ì¤˜"
    â†’ {{
        "ì¥ì†Œ": "ì œì£¼ì‹œ êµ¬ì¢Œì",
        "Categories": "í•´ë¬¼,ìƒì„ ìš”ë¦¬",
        "ì‹œê°„ëŒ€": ["ì•„ì¹¨"],
        "ì„±ë³„": [],
        "ì¡°ê±´": ["ì´ìš©ê±´ìˆ˜ ìƒìœ„ 20%", "í˜„ì§€ì¸ ì´ìš© ë¹„ì¤‘ ê°€ì¥ ë†’ìŒ"],
        "í‚¤ì›Œë“œ": ["ì¬ë£Œê°€ ì‹ ì„ í•´ìš”", "ì‹ì´ ìˆì–´ìš”"]
    }}
    8. ì…ë ¥: "ì„œê·€í¬ì‹œ ëŒ€ì •ìì—ì„œ 30ëŒ€ê°€ ë§ì´ ê°€ê³  íšŒì‹í•˜ê¸° ì¢‹ì€ ì‹ë‹¹ ì•Œë ¤ì¤˜"
    â†’ {{
        "ì¥ì†Œ": "ì„œê·€í¬ì‹œ ëŒ€ì •ì",
        "Categories": "ë‹¨í’ˆìš”ë¦¬ ì „ë¬¸",
        "ì‹œê°„ëŒ€": ["ì €ë…"],
        "ì„±ë³„": [],
        "ì¡°ê±´": ["30ëŒ€ ì´ìš© ë¹„ì¤‘ 30% ì´ìƒ", "ì´ìš©ê±´ìˆ˜ ìƒìœ„ 20%"],
        "í‚¤ì›Œë“œ": ["ë‹¨ì²´ëª¨ì„ í•˜ê¸° ì¢‹ì•„ìš”", "ë§¤ì¥ì´ ë„“ì–´ìš”"]
    }}
    9. ì…ë ¥: "ì œì£¼ì‹œ í™”ë¶ë™ì—ì„œ ì£¼ë§ì— ì¸ê¸°ìˆê³  ê°€ì¡±ë“¤ì´ ìì£¼ê°€ëŠ” ë°¥ì§‘ ì¶”ì²œí•´ì¤˜"
    â†’ {{
        "ì¥ì†Œ": "ì œì£¼ì‹œ í™”ë¶ë™",
        "Categories": "ê°€ì •ì‹",
        "ì‹œê°„ëŒ€": [],
        "ì„±ë³„": [],
        "ì¡°ê±´": ["50ëŒ€ ì´ìš© ë¹„ì¤‘ 30% ì´ìƒ", "í† ìš”ì¼ ì´ìš© ë¹„ì¤‘ ê°€ì¥ ë†’ìŒ"],
        "í‚¤ì›Œë“œ": ["ì°¨í•˜ê¸° í¸í•´ìš”", "ê°€ì¡±ëª¨ì„ í•˜ê¸° ì¢‹ì•„ìš”"]
    }}
    10. ì…ë ¥: "ì„œê·€í¬ì‹œ ì„œê·€ë™ì—ì„œ ë°¤ëŠ¦ê²Œê¹Œì§€ í•˜ê³  ì Šì€ì¸µì´ ì—¬ìë“¤ì´ ë§ì´ ê°€ëŠ” ë§›ìˆëŠ” ê³ ê¸°ì§‘ ì•Œë ¤ì¤˜"
    â†’ {{
        "ì¥ì†Œ": "ì„œê·€í¬ì‹œ ì„œê·€ë™",
        "Categories": "ìœ¡ë¥˜,ê³ ê¸°ìš”ë¦¬",
        "ì‹œê°„ëŒ€": ["ë°¤"],
        "ì„±ë³„": ["ì—¬ì„± ì´ìš© ë¹„ì¤‘ ë†’ìŒ"],
        "ì¡°ê±´": ["20ëŒ€ ì´ìš© ë¹„ì¤‘ 30% ì´ìƒ", "ì´ìš©ê±´ìˆ˜ ìƒìœ„ 10%", "ê°€ì„±ë¹„"],
        "í‚¤ì›Œë“œ": ["ìŒì‹ì´ ë§›ìˆì–´ìš”", "ê³ ê¸° ì§ˆì´ ì¢‹ì•„ìš”"]
    }}
    í˜„ì¬ ì…ë ¥: {user_input}
    ìœ„ ì˜ˆì‹œë“¤ì„ ì°¸ê³ í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
    """
    
    try:
        logging.info(f"\n{'='*50}\nLLM ìš”ì²­ ì‹œì‘\n{'='*50}")
        logging.info(f"ì‚¬ìš©ì ì…ë ¥: {user_input}")
        
        total_tokens_prompt = model.count_tokens(prompt)
        logging.info(f"ì…ë ¥ í† í° ìˆ˜: {total_tokens_prompt}")

        # íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¶”ê°€
        response = model.generate_content(
            prompt,
        )
        
        if not response or not response.text:
            logging.error("LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return None
            
        json_str = response.text.strip().replace("```json", "").replace("```", "").strip()
        logging.info(f"LLM ì‘ë‹µ: {json_str}")
        
        parsed_input = eval(json_str)
        if "ì¥ì†Œ" in parsed_input and "Categories" in parsed_input:
            logging.info(f"íŒŒì‹± ì„±ê³µ: {parsed_input}")
            return parsed_input
            
    except Exception as e:
        logging.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
        return None

# ì±„ ê¸°ë¡ ì´ˆê¸°í™” í•¨ìˆ˜
def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë–¤ ì‹ë‹¹ì„ ì°¾ìœ¼ì‹œë‚˜ìš”?"}]
    logging.info("ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”")

def get_next_time_period(current_time):
    """í˜„ì¬ ì‹œê°„ëŒ€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ì¶”ì²œí•  ì‹œê°„ëŒ€ë¥¼ ë°˜í™˜"""
    time_flow = {
        "ì•„ì¹¨": "ì ì‹¬",
        "ì ì‹¬": "ì˜¤í›„",
        "ì˜¤í›„": "ì €ë…",
        "ì €ë…": "ë°¤"
    }
    return time_flow.get(current_time)

def suggest_next_destination(current_df, time_periods, location):
    """ë‹¤ìŒ ì‹œê°„ëŒ€ì˜ ì£¼ë³€ ë§›ì§‘ì„ ì¶”ì²œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        # ì…ë ¥ê°’ ìœ íš¨ì„± ê²€ì‚¬ ì¶”ê°€
        if current_df is None or current_df.empty:
            logging.warning("í˜„ì¬ ì¶”ì²œ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return None, None
            
        if not time_periods:  # ì‹œê°„ëŒ€ê°€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
            logging.warning("ì‹œê°„ëŒ€ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None
            
        current_time = time_periods[0]  # í˜„ì¬ ì‹œê°„ëŒ€
        next_time = get_next_time_period(current_time)
        
        if not next_time:  # ë‹¤ìŒ ì‹œê°„ëŒ€ê°€ ì—†ëŠ” ê²½ìš°
            logging.warning(f"'{current_time}' ë‹¤ìŒ ì‹œê°„ëŒ€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None
            
        logging.info(f"ë‹¤ìŒ ë™ì„  ì¶”ì²œ ì‹œì‘ - í˜„ì¬ ì‹œê°„ëŒ€: {current_time}, ë‹¤ìŒ ì‹œê°„ëŒ€: {next_time}")
        
        # í˜„ì¬ ì¶”ì²œëœ ì‹ë‹¹ë“¤ì˜ ì¤‘ì‹¬ ì¢Œí‘œ ê³„ì‚°
        center_coords = None
        for _, row in current_df.head().iterrows():
            coords = get_coords_from_address(row['ì£¼ì†Œ'])
            if coords:
                if center_coords is None:
                    center_coords = coords
                else:
                    center_coords['lat'] = (center_coords['lat'] + coords['lat']) / 2
                    center_coords['lng'] = (center_coords['lng'] + coords['lng']) / 2
        
        if not center_coords:
            return None, None
            
        # ì£¼ë³€ ì‹ë‹¹ í•„í„°ë§ì„ ìœ„í•œ ì„ì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
        filtered_df = filter_data_by_criteria(
            df=df,  # ì „ì—­ ë°ì´í„°í”„ë ˆì„
            location=location,  # í˜„ì¬ ìœ„ì¹˜
            category=None,  # ì¹´í…Œê³ ë¦¬ ì œí•œ ì—†ìŒ
            time_periods=[next_time],  # ë‹¤ìŒ ì‹œê°„ëŒ€
            conditions=None,
            gender_preference=None
        )
        
        if filtered_df is None or filtered_df.empty:
            return None, None
            
        # ì‘ë‹µ ìƒì„±
        next_destination_response = f"""
        ### ğŸ•’ ë‹¤ìŒ {next_time} ì‹œê°„ëŒ€ ì¶”ì²œ ë§›ì§‘

        í˜„ì¬ ìœ„ì¹˜ ê·¼ì²˜ì—ì„œ {next_time}ì— ë°©ë¬¸í•˜ê¸° ì¢‹ì€ ë§›ì§‘ì„ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤:
        """
        
        # ìƒˆë¡œìš´ Gemini í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""
        ë‹¤ìŒ ì‹œê°„ëŒ€({next_time})ì— ë°©ë¬¸í•˜ê¸° ì¢‹ì€ ì£¼ë³€ ë§›ì§‘ë“¤ì˜ íŠ¹ì§•ì„ ìš”ì•½í•´ì„œ ì¶”ì²œí•´ì£¼ì„¸ìš”.
        
        ì¶”ì²œí•  ì‹ë‹¹ ì •ë³´:
        {filtered_df.head().to_dict('records')}
        
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
        1. ì „ë°˜ì ì¸ íŠ¹ì§• ì†Œê°œ
        2. ê° ì‹ë‹¹ë³„ í•µì‹¬ íŠ¹ì§• (1-2ì¤„)
        3. ë§ˆë¬´ë¦¬ ë©˜íŠ¸
        """
        
        response = model.generate_content(prompt)
        if response and response.text:
            next_destination_response += response.text
            
        return next_destination_response, filtered_df
        
    except Exception as e:
        logging.error(f"ë‹¤ìŒ ë™ì„  ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None, None

# Streamlit UI ì„¤ì •
st.title("ğŸŠ ì œì£¼ ë§›ì§‘ ì¶”ì²œ ì±—ë´‡")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë¶€ë¶„
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë–¤ ì‹ë‹¹ì„ ì°¾ìœ¼ì‹œë‚˜ìš”?"}]
    
if "map_data" not in st.session_state:
    st.session_state.map_data = None

# ìƒíƒœ ë³€ìˆ˜ ì¶”ê°€
if 'current_recommendation' not in st.session_state:
    st.session_state.current_recommendation = None
if 'next_recommendation' not in st.session_state:
    st.session_state.next_recommendation = None

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ë¶€ë¶„
if prompt := st.chat_input("ì˜ˆ: ì•„ì¹¨ì— 20ëŒ€ì¸ ë‚¨ìì¸ ì¹œêµ¬ë“¤ê³¼ ì• ì›”ì— í•´ì‚°ë¬¼ì§‘ì„ ê°ˆ ê±°ì•¼"):
    try:
        logging.info("\n" + "="*50)
        logging.info("ìƒˆë¡œìš´ ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬ ì‹œì‘")
        logging.info(f"ì‚¬ìš©ì ì…ë ¥: {prompt}")

        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        with st.chat_message("user"):
            st.write(prompt)
            st.session_state.messages.append({"role": "user", "content": prompt})

        parsed_input = parse_user_input_to_json(prompt)
        logging.info(f"íŒŒì‹±ëœ JSON ê²°ê³¼: {parsed_input}")

        if parsed_input:
            location = parsed_input["ì¥ì†Œ"]
            category = parsed_input["Categories"]
            time = parsed_input.get("ì‹œê°„ëŒ€", [])
            conditions = parsed_input.get("ì¡°ê±´", [])
            keywords = parsed_input.get("í‚¤ì›Œë“œ", [])
            gender_preference = parsed_input.get("ì„±ë³„", [])

            # ê¸°ë³¸ í•„í„°ë§
            result_df = filter_data_by_criteria(df, location, category, time_periods=time, conditions=conditions, gender_preference=gender_preference)

            # í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§
            if keywords and not result_df.empty:
                result_df = filter_by_keywords(result_df, keywords)

            # ì²« ë²ˆì§¸ ì‘ë‹µ ì €ì¥
            st.session_state.current_recommendation = {
                'response': generate_response(
                    prompt=prompt,
                    result_df=result_df,
                    gender_preference=gender_preference,
                    time=", ".join(time) if time else "",
                    category_type=category,
                    keywords=keywords
                ),
                'df': result_df.copy()  # DataFrame ë³µì‚¬ë³¸ ì €ì¥
            }

            # ë‹¤ìŒ ì¶”ì²œ ì¤€ë¹„
            if time and result_df is not None and not result_df.empty:
                next_suggestion_result = suggest_next_destination(result_df, time, location)
                if next_suggestion_result is not None:
                    next_suggestion, next_df = next_suggestion_result
                    if next_suggestion and next_df is not None and not next_df.empty:
                        st.session_state.next_recommendation = {
                            'response': next_suggestion,
                            'df': next_df.copy()
                        }
                    else:
                        st.session_state.next_recommendation = None
                else:
                    st.session_state.next_recommendation = None
            else:
                st.session_state.next_recommendation = None

            # ì‘ë‹µì„ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            with st.chat_message("assistant"):
                st.markdown(st.session_state.current_recommendation['response'])
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": st.session_state.current_recommendation['response']
                })

        else:
            with st.chat_message("assistant"):
                error_message = "ì¥ì†Œì™€ ì—…ì¢…ì„ ì •í™•íˆ ì…ë ¥í•´ì£¼ì„¸ìš”."
                st.warning(error_message)
                st.session_state.messages.append({"role": "assistant", "content": error_message})

    except Exception as e:
        with st.chat_message("assistant"):
            error_message = "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            st.error(error_message)
            st.session_state.messages.append({"role": "assistant", "content": error_message})
        logging.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)

# ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ì¶”ì²œ ê²°ê³¼ í‘œì‹œ (ì±„íŒ… UI ì™¸ë¶€)
if st.session_state.current_recommendation:
    st.markdown("---")
    st.markdown("## ğŸ½ï¸ ë§›ì§‘ ì¶”ì²œ ê²°ê³¼")
    
    # íƒ­ ìƒì„±
    tab1, tab2 = st.tabs(["ğŸ“ ë§›ì§‘ ì§€ë„", "ğŸ•’ ë‹¤ìŒ ì‹œê°„ëŒ€ ì¶”ì²œ"])
    
    # ì²« ë²ˆì§¸ íƒ­: í˜„ì¬ ì¶”ì²œ (ë‹¤ìŒ ì‹œê°„ëŒ€ ì •ë³´ í¬í•¨)
    with tab1:
        if st.session_state.current_recommendation['df'] is not None:
            next_df = None
            if st.session_state.next_recommendation and st.session_state.next_recommendation['df'] is not None:
                next_df = st.session_state.next_recommendation['df']
            
            current_map = display_map(
                st.session_state.current_recommendation['df'],
                next_df
            )
            if current_map:
                st_folium(current_map, width=700, height=500)
    
    # ë‘ ë²ˆì§¸ íƒ­: ë‹¤ìŒ ì‹œê°„ëŒ€ ì¶”ì²œ (ì§€ë„ ì—†ì´ í…ìŠ¤íŠ¸ë§Œ)
    with tab2:
        if st.session_state.next_recommendation:
            st.markdown(st.session_state.next_recommendation['response'])
        else:
            st.info("ë‹¤ìŒ ì‹œê°„ëŒ€ ì¶”ì²œì´ ì—†ìŠµë‹ˆë‹¤.")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.markdown("### ğŸ—ºï¸ ì§€ë„ ì„¤ëª…")
st.sidebar.markdown("""
- ğŸ”´ **ë¹¨ê°„ìƒ‰**: í˜„ì¬ ì¶”ì²œ ë§›ì§‘
- ğŸ”µ **íŒŒë€ìƒ‰**: ë‹¤ìŒ ì‹œê°„ëŒ€ ì¶”ì²œ ë§›ì§‘
    - *ë‹¤ìŒ ì‹œê°„ëŒ€ ì¶”ì²œì€ ì‹œê°„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ í‘œì‹œë©ë‹ˆë‹¤*
""")

# êµ¬ë¶„ì„  ì¶”ê°€
st.sidebar.markdown("---")

# ì´ˆê¸°í™” ë²„íŠ¼
if st.sidebar.button('ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì§€ìš°ê¸°'):
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë–¤ ì‹ë‹¹ì„ ì°¾ìœ¼ì‹œë‚˜ìš”?"}]
    st.session_state.current_recommendation = None
    st.session_state.next_recommendation = None
    st.experimental_rerun()


